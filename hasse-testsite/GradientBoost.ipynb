{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b1ede8-aea9-45dd-bba7-1544094c8d51",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03895744-a8d1-49a3-b22d-df2aa6839643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3021ba-048b-4149-813f-3c0ea76ae113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332f5c3-8cd6-443e-bae0-f223afb0424f",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02590964-5c5d-4a94-8bb4-62a5da92e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a36b78d-cc75-45de-be1f-a3798ae00918",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270667f-ae20-48b4-b038-e5dc965464c0",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d55393-afa5-4e09-bcf7-f78cc71e5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(val_data, one_hot):\n",
    "    train, test, metadata = load_all_data()\n",
    "    nonCategorical, categorical = get_cat_and_non_cat_data(metadata)\n",
    "#     categorical.remove('district')\n",
    "    all_features = list(train.columns)\n",
    "    all_features.remove('price')\n",
    "    numerical_features = ['area_total','area_kitchen','area_living','floor','rooms','ceiling',\n",
    "        'bathrooms_shared','bathrooms_private','balconies','loggias','phones','building_id','constructed','stories']\n",
    "    categorical_to_numerical(train, ['street','address'])\n",
    "    categorical_to_numerical(test, ['street','address'])\n",
    "    if not val_data:\n",
    "        X_train, y_train, test_labels = pre_process_numerical(\n",
    "                            features = all_features, numerical_features = numerical_features, train = train, test = test,\n",
    "                            outliers_value=7, val_data=val_data, val_split=0.2, random_state=42, scaler=\"std\",\n",
    "                            add_R=\"True\", add_rel_height=\"True\", droptable=[],\n",
    "                            one_hot_encode=one_hot, cat_features=categorical, drop_old=True)\n",
    "        y_train_log = np.log(y_train)\n",
    "        return X_train, y_train, y_train_log, test_labels\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test, test_labels = pre_process_numerical(features = all_features, numerical_features = numerical_features, train = train, test = test,\n",
    "                            outliers_value=7, val_data=val_data, val_split=0.2, random_state=42, scaler=\"std\",\n",
    "                            add_R=\"True\", add_rel_height=\"True\", droptable=[],\n",
    "                            one_hot_encode=one_hot, cat_features=categorical, drop_old=True)\n",
    "        y_train_log = np.log(y_train)\n",
    "        return X_train, y_train, y_train_log, X_test, y_test, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37be312d-0237-413c-9cac-d7474fd3ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot encoding\n",
      "Std\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, y_train_log, test_labels = load_data(val_data=False, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16d03f-0337-4d5f-bd76-ca06fec34887",
   "metadata": {},
   "source": [
    "# Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f53de-709e-4c4a-bc38-506d9eb7cc10",
   "metadata": {},
   "source": [
    "## Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d3e51c-490a-49f3-abd0-d774d000b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost_regr = GradientBoostingRegressor(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=2000,\n",
    "    subsample=1.0, \n",
    "    criterion='mse', \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=2, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_depth=9, \n",
    "    min_impurity_decrease=0.0, \n",
    "    init=None, \n",
    "    random_state=0, \n",
    "    max_features=None,\n",
    "    alpha=0.9,\n",
    "    verbose=0,\n",
    "    max_leaf_nodes=None,\n",
    "    warm_start=False,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=None,\n",
    "    tol=0.0001,\n",
    "    ccp_alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1aab8-b615-4394-944b-17ea3e27891f",
   "metadata": {},
   "source": [
    "# Manual Tuning of Parameters\n",
    "Tuning based on strategy from: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf7c656-e593-4928-8668-701af4621df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1 = GradientBoostingRegressor(\n",
    "    learning_rate = 0.1,\n",
    "    loss = 'squared_error',\n",
    "    criterion = 'squared_error',\n",
    "    verbose = 0,\n",
    "    warm_start = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9e010-e955-4e2d-bcc2-e20a7739034c",
   "metadata": {},
   "source": [
    "## Start with Learning Rate 0.1\n",
    "### Determine Number of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe12ee8-8429-4159-ba3f-2e167974949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = np.linspace(10,100,10).astype(int)\n",
    "\n",
    "best_average_score = 1\n",
    "\n",
    "for n_est in n_estimators:\n",
    "    print(\"current number of trees: \", n_est)\n",
    "    params = dict(n_estimators = n_est)\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9bd20d-da26-43f7-9998-17eeefb18b7b",
   "metadata": {},
   "source": [
    "The score is still going lower with an increasing number of estimators, so we'll check if we can find an upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd531b3b-ed9f-4ca4-8c14-0b8c242d368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = np.linspace(110,200,10).astype(int)\n",
    "\n",
    "best_average_score = 0.23017944010276176\n",
    "\n",
    "for n_est in n_estimators:\n",
    "    print(\"       -----------------       \")\n",
    "    print(\"current number of trees: \", n_est)\n",
    "    params = dict(n_estimators = n_est)\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0061659-d47a-4cdb-a229-089689d1909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = np.linspace(210,300,10).astype(int)\n",
    "\n",
    "best_average_score = 0.2219789624211495\n",
    "\n",
    "for n_est in n_estimators:\n",
    "    print(\"       -----------------       \")\n",
    "    print(\"current number of trees: \", n_est)\n",
    "    params = dict(n_estimators = n_est)\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc863a7f-1454-459d-8ba6-e1d9e304d064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e1181b-262f-43f8-b2a1-cfa8ee44c0ba",
   "metadata": {},
   "source": [
    "Conclusion: with a learning rate of 0.1, [STOPPED] 300 trees are optimal. This seems like a high but perhaps still reasonably value. Tuning will for now continue with this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93bbf4e8-b866-422e-857b-70409e1a71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_estimators = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0094805-83f9-479f-a4bd-09dbfdf7777f",
   "metadata": {},
   "source": [
    "### Tuning tree-specific parameters\n",
    "#### max_depth & min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce9cee-310d-4452-a296-0e07a0469c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = np.arange(start=4,stop=16,step=2).astype(int)\n",
    "min_samples_split = np.logspace(1, 4, num=7, endpoint=True).astype(int)\n",
    "\n",
    "best_average_score = 1\n",
    "\n",
    "for m_depth in max_depth:\n",
    "    for m_s_split in min_samples_split:\n",
    "        print(\"       -----------------       \")\n",
    "        print(\"current max depth:         \", m_depth)\n",
    "        print(\"current min samples split: \", m_s_split)\n",
    "        params = dict(\n",
    "            max_depth = m_depth,\n",
    "            min_samples_split = m_s_split,\n",
    "        )\n",
    "        model = GradientBoostingRegressor(\n",
    "                    **params,\n",
    "                    n_estimators = optimal_n_estimators,\n",
    "                    learning_rate = 0.1,\n",
    "                    loss = 'squared_error',\n",
    "                    criterion = 'squared_error',\n",
    "                    verbose = 0,\n",
    "                    warm_start = False,\n",
    "                    random_state = random_state,\n",
    "                )\n",
    "        scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                                model=model,\n",
    "                                                                X_train=X_train, \n",
    "                                                                y_train=y_train_log\n",
    "                                                                )\n",
    "        print(average_score)\n",
    "        if average_score < best_average_score:\n",
    "            best_average_score = average_score\n",
    "            best_params = params\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "            print(\"scores: \", scores)\n",
    "            print(\"average score: \", average_score)\n",
    "            print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ec3cb-3327-4f5e-86dd-576443ebcd7e",
   "metadata": {},
   "source": [
    "**Result:** \n",
    "```\n",
    "scores:  [0.20500680772749033, 0.20547976952996316, 0.17665855113277418, 0.20171066397528792, 0.22715854765728632]\n",
    "average score:  0.2032028680045604\n",
    "parameters:  {'max_depth': 14, 'min_samples_split': 1000}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b46fabc-7449-406f-bf2c-1ed4537367ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_max_depth = 13\n",
    "optimal_min_samples_split = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93f2cb-315b-4ff7-89cd-7a6101b3d9cf",
   "metadata": {},
   "source": [
    "**Check** if this can be improved by increasing `max_depth`, while keeping `min_samples_split` to 1000 (this was always optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da700a23-82d2-45cf-b11f-e7395ca5504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -----------------       \n",
      "current max depth:          13\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.202723888466623\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "scores:  [0.20330882091356947, 0.20530917286186795, 0.1769646788996571, 0.20436452577377298, 0.22367224388424758]\n",
      "average score:  0.202723888466623\n",
      "parameters:  {'max_depth': 13}\n",
      "       -----------------       \n",
      "current max depth:          16\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.207592587678627\n",
      "       -----------------       \n",
      "current max depth:          17\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.20443446714202365\n"
     ]
    }
   ],
   "source": [
    "best_average_score = 0.2032028680045604\n",
    "for m_depth in [13,16,17]:\n",
    "    print(\"       -----------------       \")\n",
    "    print(\"current max depth:         \", m_depth)\n",
    "    params = dict(\n",
    "        max_depth = m_depth,\n",
    "    )\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                n_estimators = optimal_n_estimators,\n",
    "                min_samples_split = optimal_min_samples_split,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e39483-84f8-4a76-bdf1-43ddfe1ece5c",
   "metadata": {},
   "source": [
    "**Results:** \n",
    "\n",
    "**`max_depth = 13, average_score = 0.202723888466623` --> OPTIMAL**\n",
    "\n",
    "`max_depth = 15, average_score = 0.20544291334639767`\n",
    "\n",
    "`max_depth = 16, average_score = 0.207592587678627`\n",
    "\n",
    "`max_depth = 17, average_score = 0.20443446714202365`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d69733-3c88-4e19-9949-f969fb417852",
   "metadata": {},
   "source": [
    "#### min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "311dfdcd-afa2-411c-b1ee-41adaf806af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -----------------       \n",
      "current min samples leaf:          7\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-98c80208e3ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             )\n\u001b[1;32m---> 23\u001b[1;33m     scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n\u001b[0m\u001b[0;32m     24\u001b[0m                                                             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                                                             \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ntnuProjects\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36mgradient_boost_groupKFold\u001b[1;34m(number_of_splits, model, X_train, y_train)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         model.fit(\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    664\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_samples_leaf = np.arange(7,9,1).astype(int)\n",
    "\n",
    "best_average_score = 0.205\n",
    "\n",
    "for m_s_leaf in min_samples_leaf:\n",
    "    print(\"       -----------------       \")\n",
    "    print(\"current min samples leaf:         \", m_s_leaf)\n",
    "    params = dict(\n",
    "        min_samples_leaf = m_s_leaf,\n",
    "    )\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                n_estimators = optimal_n_estimators,\n",
    "                max_depth = optimal_max_depth,\n",
    "                min_samples_split = optimal_min_samples_split,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01395048-fc1c-4df3-87dd-6a969dcbf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_min_samples_leaf = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d412f-8a31-4958-a7f2-5eef972fa426",
   "metadata": {},
   "source": [
    "**Note:** using `optimal_min_leaf_samples =1` leads to a lower average score (`0.2032028680045604`) than `= 40` with an average score of `0.20446437151272606` but we chose the higher value to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623b62f-7225-43c2-9c19-34724a43554e",
   "metadata": {},
   "source": [
    "#### max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32013abf-df63-45a0-858b-c4b9ac27ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -----------------       \n",
      "current max features:          7\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.2122976175736293\n",
      "       -----------------       \n",
      "current max features:          8\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.215511875218214\n",
      "       -----------------       \n",
      "current max features:          9\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.21502492793268552\n",
      "       -----------------       \n",
      "current max features:          10\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.2141453617395653\n",
      "       -----------------       \n",
      "current max features:          11\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.211004514136596\n",
      "       -----------------       \n",
      "current max features:          12\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.2111007983431043\n"
     ]
    }
   ],
   "source": [
    "max_features = np.arange(7,13,1).astype(int)\n",
    "\n",
    "best_average_score = 0.20446437151272606\n",
    "\n",
    "for m_feature in max_features:\n",
    "    print(\"       -----------------       \")\n",
    "    print(\"current max features:         \", m_feature)\n",
    "    params = dict(\n",
    "        max_features = m_feature,\n",
    "    )\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                n_estimators = optimal_n_estimators,\n",
    "                max_depth = optimal_max_depth,\n",
    "                min_samples_split = optimal_min_samples_split,\n",
    "                min_samples_leaf = optimal_min_samples_leaf,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffa58c7c-216d-4332-a6ef-213df7cc7882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -----------------       \n",
      "current max features:          1\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.274590576242699\n",
      "       -----------------       \n",
      "current max features:          3\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.23499314773903132\n",
      "       -----------------       \n",
      "current max features:          5\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.22470103800770844\n",
      "       -----------------       \n",
      "current max features:          7\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.2122976175736293\n",
      "       -----------------       \n",
      "current max features:          9\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.21502492793268552\n",
      "       -----------------       \n",
      "current max features:          11\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.211004514136596\n",
      "       -----------------       \n",
      "current max features:          13\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.2129342508976106\n",
      "       -----------------       \n",
      "current max features:          15\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.20745598646039215\n",
      "       -----------------       \n",
      "current max features:          17\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.20519256260271107\n",
      "       -----------------       \n",
      "current max features:          19\n",
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "0.20871233603914696\n"
     ]
    }
   ],
   "source": [
    "max_features = np.arange(1,21,2).astype(int)\n",
    "\n",
    "best_average_score = 0.20446437151272606\n",
    "\n",
    "for m_feature in max_features:\n",
    "    print(\"       -----------------       \")\n",
    "    print(\"current max features:         \", m_feature)\n",
    "    params = dict(\n",
    "        max_features = m_feature,\n",
    "    )\n",
    "    model = GradientBoostingRegressor(\n",
    "                **params,\n",
    "                n_estimators = optimal_n_estimators,\n",
    "                max_depth = optimal_max_depth,\n",
    "                min_samples_split = optimal_min_samples_split,\n",
    "                min_samples_leaf = optimal_min_samples_leaf,\n",
    "                learning_rate = 0.1,\n",
    "                loss = 'squared_error',\n",
    "                criterion = 'squared_error',\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                random_state = random_state,\n",
    "            )\n",
    "    scores, average_score, _, _ = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                            model=model,\n",
    "                                                            X_train=X_train, \n",
    "                                                            y_train=y_train_log\n",
    "                                                            )\n",
    "    print(average_score)\n",
    "    if average_score < best_average_score:\n",
    "        best_average_score = average_score\n",
    "        best_params = params\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"scores: \", scores)\n",
    "        print(\"average score: \", average_score)\n",
    "        print(\"parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7558f3-2a3f-4fa6-85e4-7b4996b712d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16610cd9-c134-4de9-83bf-b01795faeba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c3669-3afe-4503-a0d8-d60e030e1524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de383da7-d742-45a8-9f80-1f1364bbc7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b93ca0-0d80-4340-8061-69999277627f",
   "metadata": {},
   "source": [
    "# New features and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15dd37be-7ca8-4247-800a-eac4f3891a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_estimators = 300\n",
    "optimal_max_depth = 13\n",
    "optimal_min_samples_split = 1000\n",
    "optimal_min_samples_leaf = 40\n",
    "optimal_max_features = 40\n",
    "optimal_subsample = 0.95\n",
    "original_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41c38f1-1ed7-4855-8601-072aa4ce4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(\n",
    "            n_estimators = optimal_n_estimators*10,\n",
    "            max_depth = optimal_max_depth,\n",
    "            min_samples_split = optimal_min_samples_split,\n",
    "            min_samples_leaf = optimal_min_samples_leaf,\n",
    "            max_features = optimal_max_features,\n",
    "            subsample = optimal_subsample,\n",
    "            learning_rate = original_learning_rate / 10,\n",
    "            loss = 'squared_error',\n",
    "            criterion = 'squared_error',\n",
    "            verbose = 0,\n",
    "            warm_start = False,\n",
    "            random_state = random_state,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d379f866-e892-4210-9295-c51e9e82d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_cv = GradientBoostingRegressor(\n",
    "            n_estimators = optimal_n_estimators*10,\n",
    "            max_depth = optimal_max_depth,\n",
    "            min_samples_split = optimal_min_samples_split,\n",
    "            min_samples_leaf = optimal_min_samples_leaf,\n",
    "            max_features = optimal_max_features,\n",
    "            subsample = optimal_subsample,\n",
    "            learning_rate = original_learning_rate / 10,\n",
    "            loss = 'squared_error',\n",
    "            criterion = 'squared_error',\n",
    "            verbose = 0,\n",
    "            warm_start = False,\n",
    "            random_state = random_state,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429e3ca2-2be1-4eb0-bfde-c33dda1597cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minMax\n"
     ]
    }
   ],
   "source": [
    "# Define the features (this is all)\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "all_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"] # All are non-ordinal\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b43642c-26fe-4390-8973-7360511182a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n",
      "Average score 0.2052103426074366\n"
     ]
    }
   ],
   "source": [
    "scores, average_score, best_model, best_index = gradient_boost_groupKFold(number_of_splits=number_of_splits,\n",
    "                                                        model=model,\n",
    "                                                        X_train=train_labels, \n",
    "                                                        y_train=np.log(train_targets)\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc6bc77a-93d8-4b05-8d02-5b6a4b120a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:         [0.21432832954051392, 0.18445627736361914, 0.1999504554104062, 0.19235765459822682, 0.23495899612441673]\n",
      "average score:  0.2052103426074366\n",
      "best model:     GradientBoostingRegressor(criterion='squared_error', learning_rate=0.01,\n",
      "                          max_depth=13, max_features=40, min_samples_leaf=40,\n",
      "                          min_samples_split=1000, n_estimators=3000,\n",
      "                          random_state=1, subsample=0.95)\n",
      "best index:     1\n"
     ]
    }
   ],
   "source": [
    "print(\"scores:        \", scores)\n",
    "print(\"average score: \", average_score)\n",
    "print(\"best model:    \", best_model)\n",
    "print(\"best index:    \", best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c38e59-04d9-4a49-bd5d-130d42bf8b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(criterion='squared_error', learning_rate=0.01,\n",
       "                          max_depth=13, max_features=40, min_samples_leaf=40,\n",
       "                          min_samples_split=1000, n_estimators=3000,\n",
       "                          random_state=1, subsample=0.95)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_cv.fit(train_labels,np.log(train_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1bdda15-5a91-4a43-aab6-1bfa77c47769",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_store(model_no_cv, test_labels, test, path=\".\\submissions\\GB4.0.csv\", exponential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35613de4-8da9-44e0-8976-36c8e48b1b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f718475-8deb-4f6c-85a9-28f3db3cc13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11bb39-4800-4796-8654-86b4b7b55d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca650e9-f7f7-46a0-beb8-7347207cd04d",
   "metadata": {},
   "source": [
    "# LOOOOT of extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e66b03-3bb2-49fa-b0ff-520775bcc069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minMax\n"
     ]
    }
   ],
   "source": [
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\", # Bool\n",
    "                     \"street\", \"address\"] # Strings\n",
    "\n",
    "all_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"] # All are non-ordinal\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(\n",
    "    train_labels, \n",
    "    test_labels,\n",
    "    add_base_features=True, \n",
    "    add_bool_features=True,\n",
    "    add_weak_features=True,\n",
    "    add_dist_to_metro=True,\n",
    "    add_close_to_uni=True,\n",
    "    add_dist_to_hospital=True,\n",
    "    add_floor_features=True,\n",
    "    add_street_info=True,\n",
    "    )\n",
    "\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79af92b-81a9-409e-b8c3-b8bcb73660f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "droptable = ['street','address']\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2c9717-d7c8-418f-bcaf-d9ebcfe454b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_estimators = 300\n",
    "optimal_max_depth = 13\n",
    "optimal_min_samples_split = 1000\n",
    "optimal_min_samples_leaf = 40\n",
    "optimal_max_features = 40\n",
    "optimal_subsample = 0.95\n",
    "original_learning_rate = 0.1\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "            n_estimators = optimal_n_estimators*10,\n",
    "            max_depth = optimal_max_depth,\n",
    "            min_samples_split = optimal_min_samples_split,\n",
    "            min_samples_leaf = optimal_min_samples_leaf,\n",
    "            max_features = optimal_max_features,\n",
    "            subsample = optimal_subsample,\n",
    "            learning_rate = original_learning_rate / 10,\n",
    "            loss = 'squared_error',\n",
    "            criterion = 'squared_error',\n",
    "            verbose = 0,\n",
    "            warm_start = False,\n",
    "            random_state = random_state,\n",
    "        )\n",
    "\n",
    "model_no_cv = GradientBoostingRegressor(\n",
    "            n_estimators = optimal_n_estimators*10,\n",
    "            max_depth = optimal_max_depth,\n",
    "            min_samples_split = optimal_min_samples_split,\n",
    "            min_samples_leaf = optimal_min_samples_leaf,\n",
    "            max_features = optimal_max_features,\n",
    "            subsample = optimal_subsample,\n",
    "            learning_rate = original_learning_rate / 10,\n",
    "            loss = 'squared_error',\n",
    "            criterion = 'squared_error',\n",
    "            verbose = 0,\n",
    "            warm_start = False,\n",
    "            random_state = random_state,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19cc802-2255-4627-8a96-2ed03837e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n",
      "starting on split  5  of cross validation\n"
     ]
    }
   ],
   "source": [
    "scores, average_score, best_model, best_index = gradient_boost_groupKFold(\n",
    "    number_of_splits=number_of_splits,\n",
    "    model=model,\n",
    "    X_train=train_labels, \n",
    "    y_train=np.log(train_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733fb52b-1eea-4bcc-a06c-c44ec775db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:         [0.2152763450188409, 0.1795831890053143, 0.1984155296145181, 0.1920902203136464, 0.23641330039989722]\n",
      "average score:  0.20435571687044338\n",
      "best model:     GradientBoostingRegressor(criterion='squared_error', learning_rate=0.01,\n",
      "                          max_depth=13, max_features=40, min_samples_leaf=40,\n",
      "                          min_samples_split=1000, n_estimators=3000,\n",
      "                          random_state=1, subsample=0.95)\n",
      "best index:     1\n"
     ]
    }
   ],
   "source": [
    "print(\"scores:        \", scores)\n",
    "print(\"average score: \", average_score)\n",
    "print(\"best model:    \", best_model)\n",
    "print(\"best index:    \", best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f14f00-0f5d-4450-b95c-1805c0595b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_cv.fit(train_labels,np.log(train_targets))\n",
    "predict_and_store(model_no_cv, test_labels, test, path=\".\\submissions\\GB5.0.csv\", exponential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3100b56-bc93-4991-85c3-ec6da8459167",
   "metadata": {},
   "source": [
    "## Predict price per square meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed27de23-298c-4b21-b4af-87cc3f75d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_per_square_meter = train_targets/train['area_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f70ca3-c1a2-4ad6-8def-d718548781d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting on split  1  of cross validation\n",
      "starting on split  2  of cross validation\n",
      "starting on split  3  of cross validation\n",
      "starting on split  4  of cross validation\n"
     ]
    }
   ],
   "source": [
    "scores, average_score, best_model, best_index = gradient_boost_groupKFold(\n",
    "    number_of_splits=number_of_splits,\n",
    "    model=model,\n",
    "    X_train=train_labels.drop(['area_total'],axis=1), \n",
    "    y_train=np.log(price_per_square_meter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed35994-afb6-4a91-afb4-38bf3efe1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_cv.fit(train_labels.drop(['area_total'],axis=1),np.log(price_per_square_meter))\n",
    "predict_and_store(model_no_cv, test_labels.drop(['area_total'],axis=1), test, path=\".\\submissions\\GB5.1.csv\", exponential=True, price_per_sq = True, total_area_df = test['area_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf3e4c-cc69-46f8-94c1-e7f5e0ab1a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
