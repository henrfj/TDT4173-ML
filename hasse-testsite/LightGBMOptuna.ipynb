{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb4deb3-4bef-4fef-a64b-f8b1d254e2b6",
   "metadata": {},
   "source": [
    "**Reference:** https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a8ca182-08dc-4e65-9c4a-ef722938cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common_utils.py\n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9531f0a0-a32a-492d-ac61-68f2261ddd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "number_of_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf27b416-4a8f-4366-884f-4bba0030702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std\n"
     ]
    }
   ],
   "source": [
    "def load_data(val_data=False):\n",
    "    train, test, metadata = load_all_data()\n",
    "    nonCategorical, categorical = get_cat_and_non_cat_data(metadata)\n",
    "    categorical.remove('district')\n",
    "    all_features = list(train.columns)\n",
    "    all_features.remove('price')\n",
    "    numerical_features = ['area_total','area_kitchen','area_living','floor','rooms','ceiling',\n",
    "        'bathrooms_shared','bathrooms_private','balconies','loggias','phones','building_id','constructed','stories']\n",
    "    categorical_to_numerical(train, ['street','address'])\n",
    "    categorical_to_numerical(test, ['street','address'])\n",
    "    if not val_data:\n",
    "        X_train, y_train, test_labels = pre_process_numerical(features = all_features, numerical_features = numerical_features, train = train, test = test,\n",
    "                            outliers_value=7, val_data=val_data, val_split=0.2, random_state=42, scaler=\"std\",\n",
    "                            add_R=\"True\", add_rel_height=\"True\", droptable=[],\n",
    "                            one_hot_encode=False, cat_features=categorical, drop_old=True)\n",
    "        y_train_log = np.log(y_train)\n",
    "        return X_train, y_train, y_train_log, test_labels\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test, test_labels = pre_process_numerical(features = all_features, numerical_features = numerical_features, train = train, test = test,\n",
    "                            outliers_value=7, val_data=val_data, val_split=0.2, random_state=42, scaler=\"std\",\n",
    "                            add_R=\"True\", add_rel_height=\"True\", droptable=[],\n",
    "                            one_hot_encode=False, cat_features=categorical, drop_old=True)\n",
    "        y_train_log = np.log(y_train)\n",
    "        return X_train, y_train, y_train_log, X_test, y_test, test_labels\n",
    "X_train, y_train, y_train_log, test_labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06692e6c-02a1-4e8c-a603-b22e0cc7cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    \n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"num_iterations\": trial.suggest_int(\"num_iterations\",1e2,1e5,log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1e1,1e4,log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3,log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 5, 2000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.5, 0.95, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1,11,step=1),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    scores = []\n",
    "        \n",
    "    cv = GroupKFold(n_splits=number_of_splits)\n",
    "    groups = X_train[\"building_id\"]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        for train_index, test_index in cv.split(X_train, y_train, groups):\n",
    "            X_train2, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train2, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            model = lgbm.LGBMRegressor(**param_grid, random_state=random_state)\n",
    "\n",
    "            model.fit(\n",
    "                X_train2,\n",
    "                y_train2,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric='rmse',\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=100,\n",
    "                callbacks = [LightGBMPruningCallback(trial, 'rmse')],\n",
    "            )\n",
    "            prediction = np.exp(model.predict(X_test))\n",
    "            score = root_mean_squared_log_error(prediction, np.exp(y_test))\n",
    "            scores.append(score)\n",
    "    return np.average(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "897e586e-71a3-491e-88f6-f96db43ca6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-08 17:02:48,432]\u001b[0m A new study created in memory with name: LGBM Classifier\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:34,840]\u001b[0m Trial 0 finished with value: 0.3892327397815446 and parameters: {'num_iterations': 28575, 'n_estimators': 107, 'learning_rate': 0.0012743551434119557, 'num_leaves': 1245, 'max_depth': 6, 'min_data_in_leaf': 3100, 'lambda_l1': 30, 'lambda_l2': 80, 'min_gain_to_split': 8.108115609100546, 'bagging_fraction': 0.6, 'bagging_freq': 5, 'feature_fraction': 0.2}. Best is trial 0 with value: 0.3892327397815446.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:35,467]\u001b[0m Trial 1 finished with value: 0.8433245390866679 and parameters: {'num_iterations': 4653, 'n_estimators': 134, 'learning_rate': 0.04773111221019114, 'num_leaves': 525, 'max_depth': 17, 'min_data_in_leaf': 9500, 'lambda_l1': 55, 'lambda_l2': 35, 'min_gain_to_split': 7.987673188857646, 'bagging_fraction': 0.5, 'bagging_freq': 1, 'feature_fraction': 0.8}. Best is trial 0 with value: 0.3892327397815446.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:36,595]\u001b[0m Trial 2 finished with value: 0.3542759590307547 and parameters: {'num_iterations': 110, 'n_estimators': 87, 'learning_rate': 0.15924812542087857, 'num_leaves': 565, 'max_depth': 15, 'min_data_in_leaf': 2200, 'lambda_l1': 85, 'lambda_l2': 0, 'min_gain_to_split': 12.543857329383115, 'bagging_fraction': 0.7, 'bagging_freq': 3, 'feature_fraction': 0.30000000000000004}. Best is trial 2 with value: 0.3542759590307547.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:41,017]\u001b[0m Trial 3 finished with value: 0.3073618163289253 and parameters: {'num_iterations': 13878, 'n_estimators': 1057, 'learning_rate': 0.012287285712206625, 'num_leaves': 65, 'max_depth': 17, 'min_data_in_leaf': 1300, 'lambda_l1': 15, 'lambda_l2': 35, 'min_gain_to_split': 10.37237356962282, 'bagging_fraction': 0.6, 'bagging_freq': 3, 'feature_fraction': 0.5}. Best is trial 3 with value: 0.3073618163289253.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:41,616]\u001b[0m Trial 4 finished with value: 0.8433245390866679 and parameters: {'num_iterations': 103, 'n_estimators': 118, 'learning_rate': 0.04534745916895744, 'num_leaves': 585, 'max_depth': 9, 'min_data_in_leaf': 9700, 'lambda_l1': 90, 'lambda_l2': 45, 'min_gain_to_split': 4.116448136714554, 'bagging_fraction': 0.8, 'bagging_freq': 3, 'feature_fraction': 0.2}. Best is trial 3 with value: 0.3073618163289253.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:41,734]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:41,897]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:42,206]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 101.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:42,562]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 101.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:42,701]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:43,164]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 101.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:44,660]\u001b[0m Trial 11 finished with value: 0.3038113613835657 and parameters: {'num_iterations': 410, 'n_estimators': 766, 'learning_rate': 0.26609462476026646, 'num_leaves': 65, 'max_depth': 15, 'min_data_in_leaf': 200, 'lambda_l1': 60, 'lambda_l2': 0, 'min_gain_to_split': 14.49241326077604, 'bagging_fraction': 0.7, 'bagging_freq': 6, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.3038113613835657.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:46,134]\u001b[0m Trial 12 finished with value: 0.28817751691536486 and parameters: {'num_iterations': 542, 'n_estimators': 740, 'learning_rate': 0.2905682138548763, 'num_leaves': 145, 'max_depth': 16, 'min_data_in_leaf': 200, 'lambda_l1': 60, 'lambda_l2': 20, 'min_gain_to_split': 13.964933886268167, 'bagging_fraction': 0.7, 'bagging_freq': 7, 'feature_fraction': 0.6000000000000001}. Best is trial 12 with value: 0.28817751691536486.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:47,583]\u001b[0m Trial 13 finished with value: 0.2929771118869705 and parameters: {'num_iterations': 522, 'n_estimators': 583, 'learning_rate': 0.267851869995917, 'num_leaves': 5, 'max_depth': 12, 'min_data_in_leaf': 500, 'lambda_l1': 65, 'lambda_l2': 15, 'min_gain_to_split': 14.400795806359675, 'bagging_fraction': 0.8, 'bagging_freq': 7, 'feature_fraction': 0.6000000000000001}. Best is trial 12 with value: 0.28817751691536486.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:49,150]\u001b[0m Trial 14 finished with value: 0.2873614334641753 and parameters: {'num_iterations': 669, 'n_estimators': 444, 'learning_rate': 0.14046319634459542, 'num_leaves': 365, 'max_depth': 12, 'min_data_in_leaf': 500, 'lambda_l1': 65, 'lambda_l2': 15, 'min_gain_to_split': 14.66837136723361, 'bagging_fraction': 0.8, 'bagging_freq': 8, 'feature_fraction': 0.7}. Best is trial 14 with value: 0.2873614334641753.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:49,348]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:49,559]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:49,821]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 7.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:50,047]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-08 17:03:50,286]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "with io.capture_output() as captured:\n",
    "    func = lambda trial: objective(trial, X_train, y_train_log)\n",
    "    study.optimize(func, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc875b75-c020-4c8d-8ff7-a03e9a7442b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest value (rmse): 0.28736\n",
      "\tBest params:\n",
      "\t\tnum_iterations: 669\n",
      "\t\tn_estimators: 444\n",
      "\t\tlearning_rate: 0.14046319634459542\n",
      "\t\tnum_leaves: 365\n",
      "\t\tmax_depth: 12\n",
      "\t\tmin_data_in_leaf: 500\n",
      "\t\tlambda_l1: 65\n",
      "\t\tlambda_l2: 15\n",
      "\t\tmin_gain_to_split: 14.66837136723361\n",
      "\t\tbagging_fraction: 0.8\n",
      "\t\tbagging_freq: 8\n",
      "\t\tfeature_fraction: 0.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e1450-61f5-44bd-9391-dedba2e61460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84e73747-447a-4ae8-9083-63b042dae757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest value (rmse): 0.26229\n",
      "\tBest params:\n",
      "\t\tnum_iterations: 405\n",
      "\t\tn_estimators: 4596\n",
      "\t\tlearning_rate: 0.22799605867389866\n",
      "\t\tnum_leaves: 65\n",
      "\t\tmax_depth: 12\n",
      "\t\tmin_data_in_leaf: 200\n",
      "\t\tlambda_l1: 80\n",
      "\t\tlambda_l2: 5\n",
      "\t\tmin_gain_to_split: 2.2609116007082672\n",
      "\t\tbagging_fraction: 0.9\n",
      "\t\tbagging_freq: 7\n",
      "\t\tfeature_fraction: 0.7\n"
     ]
    }
   ],
   "source": [
    "# print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "# print(f\"\\tBest params:\")\n",
    "\n",
    "# for key, value in study.best_params.items():\n",
    "#     print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "23e95897-a056-4f5c-91cc-3c4383385b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    \n",
    "    param_grid = {\n",
    "        \"num_iterations\": trial.suggest_categorical(\"num_iterations\",[10000]),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1e2,1e3,log=True),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.05]),\n",
    "        \"num_leaves\": trial.suggest_categorical(\"num_leaves\", [40]),\n",
    "        \"max_depth\": trial.suggest_categorical(\"max_depth\", [10]),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20,500, step=50),\n",
    "#         \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "#         \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "#         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_categorical(\n",
    "            \"bagging_fraction\",[ 0.9]\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [5]),\n",
    "        \"feature_fraction\": trial.suggest_categorical(\n",
    "            \"feature_fraction\",[0.8]\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    scores = []\n",
    "        \n",
    "    cv = GroupKFold(n_splits=number_of_splits)\n",
    "    groups = X_train[\"building_id\"]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        for train_index, test_index in cv.split(X_train, y_train, groups):\n",
    "            X_train2, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train2, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            model = lgbm.LGBMRegressor(**param_grid, random_state=random_state,early_stopping_round=100,silent=True,metric='regression',num_threads=4)\n",
    "\n",
    "            model.fit(\n",
    "                X_train2,\n",
    "                y_train2,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric='rmse',\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=100,\n",
    "                callbacks = [LightGBMPruningCallback(trial, 'rmse')],\n",
    "            )\n",
    "            prediction = np.exp(model.predict(X_test))\n",
    "            score = root_mean_squared_log_error(prediction, np.exp(y_test))\n",
    "            scores.append(score)\n",
    "    return np.average(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84738d9b-200c-4e97-a945-6f912eb7e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-09 21:10:12,163]\u001b[0m A new study created in memory with name: LGBM Classifier\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:10:49,925]\u001b[0m Trial 0 finished with value: 0.21078099747803117 and parameters: {'num_iterations': 10000, 'n_estimators': 102, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 470, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 0 with value: 0.21078099747803117.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:11:23,376]\u001b[0m Trial 1 finished with value: 0.20877721690751275 and parameters: {'num_iterations': 10000, 'n_estimators': 312, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 320, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 1 with value: 0.20877721690751275.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:11:55,633]\u001b[0m Trial 2 finished with value: 0.20012411838768807 and parameters: {'num_iterations': 10000, 'n_estimators': 152, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 20, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:12:29,684]\u001b[0m Trial 3 finished with value: 0.20425793806434492 and parameters: {'num_iterations': 10000, 'n_estimators': 117, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 170, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:13:10,244]\u001b[0m Trial 4 finished with value: 0.21078099747803117 and parameters: {'num_iterations': 10000, 'n_estimators': 538, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 470, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:13:45,624]\u001b[0m Trial 5 finished with value: 0.20012411838768807 and parameters: {'num_iterations': 10000, 'n_estimators': 284, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 20, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:13:45,802]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:13:45,973]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:13:46,144]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:14:29,171]\u001b[0m Trial 9 finished with value: 0.2006320936978895 and parameters: {'num_iterations': 10000, 'n_estimators': 724, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 70, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:14:29,371]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:15:03,826]\u001b[0m Trial 11 finished with value: 0.20012411838768807 and parameters: {'num_iterations': 10000, 'n_estimators': 182, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 20, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:15:09,483]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 971.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:15:43,283]\u001b[0m Trial 13 finished with value: 0.20012411838768807 and parameters: {'num_iterations': 10000, 'n_estimators': 347, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 20, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:15:43,449]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:16:18,190]\u001b[0m Trial 15 finished with value: 0.20012411838768807 and parameters: {'num_iterations': 10000, 'n_estimators': 152, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 20, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'feature_fraction': 0.8}. Best is trial 2 with value: 0.20012411838768807.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:16:18,374]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:16:18,536]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:16:18,707]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2021-11-09 21:16:18,899]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 2.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "with io.capture_output() as captured:\n",
    "    func = lambda trial: objective(trial, X_train, y_train_log)\n",
    "    study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "006b7d34-bc4a-4092-8743-065273655a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest value (rmse): 0.20012\n",
      "\tBest params:\n",
      "\t\tnum_iterations: 10000\n",
      "\t\tn_estimators: 152\n",
      "\t\tlearning_rate: 0.05\n",
      "\t\tnum_leaves: 40\n",
      "\t\tmax_depth: 10\n",
      "\t\tmin_data_in_leaf: 20\n",
      "\t\tbagging_fraction: 0.9\n",
      "\t\tbagging_freq: 5\n",
      "\t\tfeature_fraction: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea9847d0-cadf-49b1-8777-3ae33db5bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_iterations': 10000,\n",
       " 'n_estimators': 152,\n",
       " 'learning_rate': 0.05,\n",
       " 'num_leaves': 40,\n",
       " 'max_depth': 10,\n",
       " 'min_data_in_leaf': 20,\n",
       " 'bagging_fraction': 0.9,\n",
       " 'bagging_freq': 5,\n",
       " 'feature_fraction': 0.8}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59596c7-ff1c-4b7e-9eef-eed1b8a7e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
