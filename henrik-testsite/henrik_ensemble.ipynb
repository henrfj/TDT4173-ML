{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - K-fold validation\n",
    "#### Sources\n",
    "- https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/\n",
    "- https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538\n",
    "\n",
    "A method to validate model more accurately than just using one set of validation data.\n",
    "Also: stacking the k-fold models could lead to better accuracy, as the result is a model that has seen and trained on all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Simple average stacking\n",
    "Use the average of the predictions over the models, instead of using just one.\n",
    "NB!\n",
    "- sklearn.model_selection.StratifiedGroupKFold! Apartments from different buildings should not be in different datasets (val, train)\n",
    "- Also - stratified: makes y data is equally represented in different datasets (train, val): can pass log(y) or something into the StratifiedGroupKFold, and then extract price from X or something.\n",
    "- Just do bagging! Average/weighted average.\n",
    "- Stacking! Wohoo. Probably smart to have a deep learning model in the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Specific tf libraries\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls = []\n",
    "tts = []\n",
    "vls = []\n",
    "vts = []\n",
    "tstl = []\n",
    "train, test, metaData = load_all_data()\n",
    "\n",
    "\n",
    "# Features 1 \"All features\"\n",
    "features =           [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", \"r\", \"rel_height\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, val_labels, val_targets, test_labels = pre_process_numerical(\n",
    "        features, numerical_features, train, test, outliers_value=7, val_split=0.1, random_state=42, scaler=\"std\",\n",
    "        add_R=\"True\", add_rel_height=\"True\", droptable=droptable,\n",
    "        one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "tls.append(train_labels)\n",
    "tts.append(train_targets)\n",
    "vls.append(val_labels)\n",
    "vts.append(val_targets)\n",
    "tstl.append(test_labels)\n",
    "\n",
    "# Features 2 \"best features\"\n",
    "features =           [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", \"r\", \"rel_height\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "# Data pre-processing\n",
    "train_labels, train_targets, val_labels, val_targets, test_labels = pre_process_numerical(\n",
    "        features, numerical_features, train, test, outliers_value=7, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "        add_R=\"True\", add_rel_height=\"True\", droptable=droptable,\n",
    "        one_hot_encode=False, cat_features=cat_features, drop_old=False)\n",
    "\n",
    "tls.append(train_labels)\n",
    "tts.append(train_targets)\n",
    "vls.append(val_labels)\n",
    "vts.append(val_targets)\n",
    "tstl.append(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "...........................................Epoch 01643: early stopping\n",
      "INFO:tensorflow:Assets written to: models/some_model_0\\assets\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".........................................................................Epoch 02373: early stopping\n",
      "INFO:tensorflow:Assets written to: models/some_model_1\\assets\n"
     ]
    }
   ],
   "source": [
    "# Creater 5 different models to stack\n",
    "# https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object\n",
    "\n",
    "model1 = create_ANN_model()\n",
    "model2 = create_ANN_model()\n",
    "#model3 = create_ANN_model()\n",
    "#model4 = create_ANN_model()\n",
    "#model5 = create_ANN_model()\n",
    "models = [model1, model2] #, model3, model4, model5]\n",
    "\n",
    "losses = []\n",
    "epochs = [10000, 10000, 1000, 1000, 2000]\n",
    "for i, model in enumerate(models):\n",
    "    train_labels = tls[i]\n",
    "    train_targets = tts[i]\n",
    "    val_labels = vls[i]\n",
    "    val_targets = vts[i]\n",
    "    test_labels = tstl[i]\n",
    "    e = epochs[i]\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=40)\n",
    "    history = model.fit(x=train_labels, y=train_targets.values,\n",
    "              validation_data=(val_labels,val_targets.values),\n",
    "              verbose=0, epochs=e, callbacks=[early_stop, PrintDot()]\n",
    "              )\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    losses.append(hist['val_loss'].tail(1))\n",
    "    model.save('models/some_model_'+str(i))\n",
    "    predict_and_store(model, test_labels, test, path='advanced_tests/stacking_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1642    0.200561\n",
      "Name: val_loss, dtype: float64, 2372    0.239887\n",
      "Name: val_loss, dtype: float64]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LaureRF</th>\n",
       "      <td>0.20015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep</th>\n",
       "      <td>0.23278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>0.19968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB1</th>\n",
       "      <td>0.23450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB1</th>\n",
       "      <td>0.23787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN1</th>\n",
       "      <td>0.35042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RMSLE\n",
       "LaureRF  0.20015\n",
       "Deep     0.23278\n",
       "GB       0.19968\n",
       "CB1      0.23450\n",
       "XGB1     0.23787\n",
       "KNN1     0.35042"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = pd.DataFrame(\n",
    "    {'LaureRF': 0.20015,\n",
    "     'Deep': 0.23278,\n",
    "     'GB': 0.19968,\n",
    "     'CB1': 0.23450,\n",
    "     'XGB1': 0.23787,\n",
    "     'KNN1' : 0.35042},\n",
    "    index=[0]\n",
    ")\n",
    "acc = acc.T\n",
    "acc.columns = ['RMSLE']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaureRF = pd.read_csv(\"ensemble_predictions/LaureRF.csv\")\n",
    "Deep = pd.read_csv(\"ensemble_predictions/Deep.csv\")\n",
    "GB = pd.read_csv(\"ensemble_predictions/GB.csv\")\n",
    "CB1 = pd.read_csv(\"ensemble_predictions/CB1.csv\")\n",
    "XGB1 = pd.read_csv(\"ensemble_predictions/XGB1.csv\")\n",
    "KNN1 = pd.read_csv(\"ensemble_predictions/KNN1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaureRF = LaureRF.sort_values(by=\"id\")\n",
    "Deep = Deep.sort_values(by=\"id\")\n",
    "GB = GB.sort_values(by=\"id\")\n",
    "CB1 = CB1.sort_values(by=\"id\")\n",
    "XGB1 = XGB1.sort_values(by=\"id\")\n",
    "KNN1 = KNN1.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaureRF_prediction = LaureRF[\"price_prediction\"].to_numpy().T\n",
    "Deep_prediction = Deep[\"price_prediction\"].to_numpy().T\n",
    "GB_prediction = GB[\"price_prediction\"].to_numpy().T\n",
    "CB1_prediction = CB1[\"price_prediction\"].to_numpy().T\n",
    "XGB1_prediction = XGB1[\"price_prediction\"].to_numpy().T\n",
    "KNN1_prediction = KNN1[\"price_prediction\"].to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prediction = np.average(\n",
    "    [LaureRF_prediction,\n",
    "     Deep_prediction,\n",
    "     GB_prediction,\n",
    "     CB1_prediction,\n",
    "     XGB1_prediction,\n",
    "     KNN1_prediction\n",
    "    ],\n",
    "    weights = 1 / acc['RMSLE'] ** 4,\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = avg_prediction\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = LaureRF['id']\n",
    "submission['price_prediction'] = result\n",
    "if len(submission['id']) != 9937:\n",
    "    raise Exception(\"Not enough rows submitted!\")\n",
    "submission.to_csv('BESTSUBMISSIONEVER', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
