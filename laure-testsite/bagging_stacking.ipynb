{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "import matplotlib.pylab as plt\n",
    "# Specific tf libraries\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all() \n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "%run ../common_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'EnsembleData/Bagging_of_bagging/'\n",
    "bags = [        \n",
    "        [\n",
    "            {\"path\": main_path + 'Bag1/LGBM7.1.csv', 'score':0.16591},\n",
    "            {\"path\": main_path + 'Bag1/sample_KNN_3.csv', 'score':0.33883}\n",
    "        ],\n",
    "        [\n",
    "            {\"path\": main_path + 'Bag2/sample_RF_15.csv', 'score':0.20015},\n",
    "            {\"path\": main_path + 'Bag2/Xgboost.csv', 'score':0.17781}\n",
    "        ],\n",
    "        [\n",
    "            {\"path\": main_path + 'Bag3/CB5.1.csv', 'score':0.17140},\n",
    "            {\"path\": main_path + 'Bag3/submission_gradientBoost.csv', 'score':0.19968}\n",
    "        ],\n",
    "        [\n",
    "            {\"path\": main_path + 'Bag4/deep_king_5_5.csv', 'score':0.20502},\n",
    "            {\"path\": main_path + 'Bag4/GB5.1.csv', 'score':0.17172}\n",
    "        ]        \n",
    "    ]\n",
    "\n",
    "tree_bagging(bags, 'Bagging_stacking_submissions/bagging_of_bagging_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'EnsembleData/Bagging_of_bagging/'\n",
    "bags = ['Bag1', 'Bag2', 'Bag3']\n",
    "d1 = {\n",
    "    'LGBM7.1.csv': 0.16591,\n",
    "    'sample_KNN_3.csv': 0.33883\n",
    "}\n",
    "d2 = {\n",
    "    'sample_RF_15.csv': 0.20015,\n",
    "    'Xgboost.csv': 0.17781\n",
    "}\n",
    "d3 = {\n",
    "    'deep_king_5_5.csv': 0.20502,\n",
    "    'submission_gradientBoost.csv': 0.19968,\n",
    "    'CB5.1.csv': 0.17140\n",
    "}\n",
    "dataframes = [d1, d2, d3]\n",
    "bags_accs = []\n",
    "scores = []\n",
    "\n",
    "scores.append(list(d1.values()))\n",
    "scores.append(list(d2.values()))\n",
    "scores.append(list(d3.values()))\n",
    "csv_bagging(scores[0], [main_path + 'Bag1/' +d for d in d1.keys()], 'bag1.csv')\n",
    "bags_accs.append(np.average(scores[0], weights = [ 1 / s ** 4 for s in scores[0]]))\n",
    "csv_bagging(scores[1], [main_path + 'Bag2/' +d for d in d2.keys()], 'bag2.csv')\n",
    "bags_accs.append(np.average(scores[1], weights = [ 1 / s ** 4 for s in scores[1]]))\n",
    "csv_bagging(scores[2], [main_path + 'Bag3/' +d for d in d3.keys()], 'bag3.csv')\n",
    "bags_accs.append(np.average(scores[2], weights = [ 1 / s ** 4 for s in scores[2]]))\n",
    "\n",
    "csv_bagging(bags_accs, ['bag1.csv', 'bag2.csv', 'bag3.csv'], 'bagging_of_bagging_3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = pd.DataFrame(d1, index=[0])\n",
    "\n",
    "acc2 = pd.DataFrame(d2, index=[0])\n",
    "\n",
    "acc3 = pd.DataFrame(d3, index=[0])\n",
    "accs = [acc1, acc2, acc3]\n",
    "for i, acc in enumerate(accs):\n",
    "    accs[i] = acc.T\n",
    "    accs[i].columns = ['RMSLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i, b in enumerate(bags):\n",
    "    ps = []\n",
    "    for p in dataframes[i].keys():\n",
    "        ps.append(pd.read_csv(main_path + b + '/' + p))\n",
    "    predictions.append(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predictions = []\n",
    "for i, b in enumerate(bags):\n",
    "    pp = []\n",
    "    for p in predictions[i]:\n",
    "        p = p.sort_values(by=\"id\")\n",
    "        pp.append(p[\"price_prediction\"].to_numpy().T)\n",
    "    price_predictions.append(pp)\n",
    "\n",
    "\n",
    "# for p in price_predictions:\n",
    "#     plt.plot(predictions[0]['id'], p)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28468528.62271312 10073944.56162484  6102518.14151483 ...\n",
      "  9515104.65053492  9474174.32690924  7205435.51588924]\n",
      "[27528028.29234725 10079861.79803519  6200631.52622813 ...\n",
      " 10413543.33993634  9050273.2430835   7099115.0019421 ]\n",
      "[29726484.3061935  10204900.22818945  6303836.77018118 ...\n",
      " 10349845.91476026 10051736.01449347  6919126.58952036]\n",
      "[28574791.06547715 10131103.63236542  6229911.21691955 ...\n",
      " 10261597.75296659  9526473.44288721  7039007.35660396]\n"
     ]
    }
   ],
   "source": [
    "bags_predictions = []\n",
    "bags_accs = []\n",
    "for i, b in enumerate(bags):\n",
    "    bags_accs.append(np.average(accs[i]))\n",
    "    avg_prediction = np.average(\n",
    "        price_predictions[i],\n",
    "        weights = 1 / accs[i]['RMSLE'] ** 4,\n",
    "        axis=0\n",
    "    )\n",
    "    print(avg_prediction)\n",
    "    bags_predictions.append(avg_prediction)\n",
    "\n",
    "bags_accs = pd.DataFrame(bags_accs, columns=['RMSLE'])\n",
    "\n",
    "result = np.average(\n",
    "    bags_predictions,\n",
    "    weights = 1 / bags_accs['RMSLE'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = predictions[0][0]['id']\n",
    "submission['price_prediction'] = result\n",
    "if len(submission['id']) != 9937:\n",
    "    raise Exception(\"Not enough rows submitted!\")\n",
    "submission.to_csv('bagging_of_bagging_1.csv', index=False)\n",
    "\n",
    "# kaggle_scores = [0.23450, 0.20502, 0.23278, 0.19968, 0.35042, 0.20015, 0.20159, 0.23787]\n",
    "# csv_paths = [\"ensemble_predictions/csvs/CB1.csv\", \"ensemble_predictions/csvs/deep_king_5_5.csv\", \"ensemble_predictions/csvs/deep.csv\", \"ensemble_predictions/csvs/GB.csv\",\n",
    "#              \"ensemble_predictions/csvs/KNN1.csv\", \"ensemble_predictions/csvs/LaureRF.csv\", \"ensemble_predictions/csvs/xgb_king_2.csv\", \"ensemble_predictions/csvs/XGB1.csv\"]\n",
    "# submission_path = \"ensemble_predictions/bagging_2\"\n",
    "\n",
    "# csv_bagging(kaggle_scores, csv_paths, submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jdbosudgbpgibsp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18644/1716262656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# stop between bagging and stacking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjdbosudgbpgibsp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'jdbosudgbpgibsp' is not defined"
     ]
    }
   ],
   "source": [
    "# stop between bagging and stacking\n",
    "jdbosudgbpgibsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, metaData = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot encoding\n",
      "minMax\n",
      "\n",
      ".................................."
     ]
    }
   ],
   "source": [
    "# Mostly the ones correlated to price.\n",
    "features = [\"building_id\",\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(\n",
    "    features, numerical_features, train, test, outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "    add_R=\"True\", add_rel_height=\"True\",  add_spacious=True, droptable=droptable,\n",
    "    one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "model_params = ([18, 12, 6], tf.nn.relu,\n",
    "                     [False, False, False], 0.2, tf.keras.optimizers.Adam,\n",
    "                      rmsle_custom, ['mse', 'msle', tf.keras.metrics.Accuracy()], True)\n",
    "\n",
    "# NB! We cannot just compile ann model beforehand, will just keep training on same model.\n",
    "ann_oof_train, ann_oof_test, hists = get_oof_ann(model_params, train_labels, train_targets, test_labels)\n",
    "\n",
    "# ADVISOR PERFORMANCE\n",
    "for histories in hists:\n",
    "    hist = pd.DataFrame(histories.history)\n",
    "    hist['epoch'] = histories.epoch\n",
    "    print(hist[\"val_loss\"].tail(1))\n",
    "print(np.sum(ann_oof_train >= 0) / len(ann_oof_train))\n",
    "print(np.sum(ann_oof_test >= 0) / len(ann_oof_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minMax\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20264/3768219413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     one_hot_encode=False, cat_features=cat_features, drop_old=True) # FALSE!\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mxgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mxgb_oof_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_oof_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof_xgboost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgboost' is not defined"
     ]
    }
   ],
   "source": [
    "# ALL, and building ID!\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude']\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=False, cat_features=cat_features, drop_old=True) # FALSE!\n",
    "\n",
    "xgb_model = xgboost.XGBRegressor(max_depth=5, min_child_weight=12, gamma=0.15, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.1, reg_lambda=0.3, learning_rate=0.01, n_estimators=10000)\n",
    "\n",
    "xgb_oof_train, xgb_oof_test, scores = get_oof_xgboost(xgb_model, train_labels, np.log(train_targets), test_labels)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11924/2386402069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_area\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillNan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Add new features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madded_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minMax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36mfeature_engineering\u001b[1;34m(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;31m# Newly_built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actually_new\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;31m# Newly_built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actually_new\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5507\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5508\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5509\u001b[1;33m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5511\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "lgbm_model = lightgbm.LGBMRegressor(\n",
    "  num_iterations= 10000,\n",
    "  n_estimators= 152,\n",
    "  learning_rate= 0.05,\n",
    "  num_leaves= 40,\n",
    "  max_depth= 10,\n",
    "  min_data_in_leaf= 20,\n",
    "  bagging_fraction= 0.9,\n",
    "  bagging_freq= 5,\n",
    "  feature_fraction= 0.8,\n",
    "  random_state=1,\n",
    "  early_stopping_round=100,\n",
    "  silent=True,\n",
    "  metric='regression',\n",
    "  num_threads=4\n",
    ")\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test, scores = get_oof_lgbm(lgbm_model, train_labels, np.log(train_targets), test_labels)\n",
    "# Advisor performance\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"building_id\",\n",
    "            \"area_total\", \"latitude\", \"longitude\", \"floor\", \"stories\", # Numerical\n",
    "            \"district\", 'condition' # Categorical\n",
    "           ]\n",
    "\n",
    "numerical = [] # No need to scale for RF! https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package\n",
    "cat_features = []\n",
    "droptable = []\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"none\",\n",
    "                    add_R=False, add_rel_height=False, add_spacious=False, droptable=droptable,\n",
    "                    one_hot_encode=False, cat_features=cat_features, drop_old=False) \n",
    "\n",
    "rf_oof_train, rf_oof_test, scores = get_oof_rf(train_labels, train_targets, test_labels) # Use log() for RF?\n",
    "\n",
    "print(scores)\n",
    "print(\"==>\\t\",np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"building_id\", # For Grouping\n",
    "            \"area_total\", \"ceiling\", \"floor\", \"bathrooms_shared\", \"balconies\", \"stories\", \"rooms\",\"latitude\",\"longitude\", # Numerical\n",
    "            \"windows_street\", \"elevator_without\", # Bool\n",
    "            \"parking\", \"heating\", \"district\", \"condition\"  # Categorical\n",
    "           ]\n",
    "\n",
    "numerical_features = [\"area_total\", \"ceiling\", \"floor\", \"bathrooms_shared\", \"balconies\", \"stories\", \"rooms\"]\n",
    "categorical_features = [\"parking\", \"heating\", \"district\", \"condition\"]\n",
    "droptable=[\"latitude\",\"longitude\"]\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=True, cat_features=cat_features, drop_old=False) \n",
    "\n",
    "knn_oof_train, knn_oof_test, scores = get_oof_knn(train_labels, train_targets, test_labels)\n",
    "\n",
    "print(scores)\n",
    "print(\"==>\\t\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL, and building ID!\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude']\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=True, cat_features=cat_features, drop_old=True) \n",
    "\n",
    "xgb_king = xgboost.XGBRegressor(max_depth=5, min_child_weight=12, gamma=0.15, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.1, reg_lambda=0.3, learning_rate=0.01, n_estimators=10000)\n",
    "\n",
    "# TODO: shouldn't xgb and lgbm predict the log(price)?\n",
    "# NB! ann predicts some negative values! Maybe RELU on the output?\n",
    "train_labels[\"xgb_preds\"] = xgb_oof_train\n",
    "train_labels[\"lgbm_preds\"] = lgbm_oof_train\n",
    "train_labels[\"ann_preds\"] = np.log(ann_oof_train)\n",
    "train_labels[\"knn_preds\"] = np.log(knn_oof_train)\n",
    "train_labels[\"rf_preds\"] = np.log(rf_oof_train)\n",
    "\n",
    "test_labels[\"xgb_preds\"] = lgbm_oof_test\n",
    "test_labels[\"lgbm_preds\"] = xgb_oof_test\n",
    "test_labels[\"ann_preds\"] = np.log(ann_oof_test)\n",
    "test_labels[\"knn_preds\"] = np.log(knn_oof_test)\n",
    "test_labels[\"rf_preds\"] = np.log(rf_oof_test)\n",
    "\n",
    "# Drop all except the good stuff?\n",
    "train_labels.drop(train_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "test_labels.drop(test_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "\n",
    "scores, avg, best_model, best_index = XGB_groupKFold(5, xgb_king, train_labels, np.log(train_targets),\n",
    "    eval_metric='rmse')\n",
    "\n",
    "# Kings performance\n",
    "print(scores)\n",
    "print(\"==>\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly the ones correlated to price.\n",
    "features =           [\"building_id\",\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(\n",
    "    features, numerical_features, train, test, outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "    add_R=\"True\", add_rel_height=\"True\",  add_spacious=True, droptable=droptable,\n",
    "    one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "train_labels[\"xgb_preds\"] = np.exp(xgb_oof_train)\n",
    "train_labels[\"lgbm_preds\"] = np.exp(lgbm_oof_train)\n",
    "train_labels[\"ann_preds\"] = ann_oof_train\n",
    "train_labels[\"knn_preds\"] = knn_oof_train\n",
    "train_labels[\"rf_preds\"] = rf_oof_train\n",
    "\n",
    "test_labels[\"xgb_preds\"] = np.exp(lgbm_oof_test)\n",
    "test_labels[\"lgbm_preds\"] = np.exp(xgb_oof_test)\n",
    "test_labels[\"ann_preds\"] = ann_oof_test\n",
    "test_labels[\"knn_preds\"] = knn_oof_test\n",
    "test_labels[\"rf_preds\"] = rf_oof_test\n",
    "\n",
    "# Drop all except the good stuff?\n",
    "train_labels.drop(train_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "test_labels.drop(test_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "\n",
    "## TODO! Use a deeper net, with regularization maybe :D\n",
    "model_params = ([18, 18, 18], tf.nn.leaky_relu,\n",
    "                     [False, False, False], 0.2, 'adam',\n",
    "                      rmsle_custom, ['mse', 'msle', tf.keras.metrics.Accuracy()], True)\n",
    "\n",
    "ann_scores, models, best_model, hists = ANN_groupKFold(5, model_params, train_labels, train_targets)\n",
    "\n",
    "print(ann_scores)\n",
    "print(\"==>\\t\", np.average(ann_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradientboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (this is all)\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "all_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"] # All are non-ordinal\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "model_no_cv = GradientBoostingRegressor(\n",
    "    n_estimators = optimal_n_estimators*10,\n",
    "    max_depth = optimal_max_depth,\n",
    "    min_samples_split = optimal_min_samples_split,\n",
    "    min_samples_leaf = optimal_min_samples_leaf,\n",
    "    max_features = optimal_max_features,\n",
    "    subsample = optimal_subsample,\n",
    "    learning_rate = original_learning_rate / 10,\n",
    "    loss = 'squared_error',\n",
    "    criterion = 'squared_error',\n",
    "    verbose = 0,\n",
    "    warm_start = False,\n",
    "    random_state = random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfoqubfouqfoq\n",
    "#stop the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((\n",
    "    dt_oof_train,\n",
    "    rf_oof_train,\n",
    "    ada_oof_train,\n",
    "    gb_oof_train,\n",
    "    lgbm_oof_train,\n",
    "    cat_oof_train\n",
    "), axis=1)\n",
    "\n",
    "x_test = np.concatenate((\n",
    "    dt_oof_test,\n",
    "    rf_oof_test,\n",
    "    ada_oof_test,\n",
    "    gb_oof_test,\n",
    "    lgbm_oof_test,\n",
    "    cat_oof_test\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "META_MODEL = lgb.LGBMRegressor(\n",
    "    num_leaves=5,\n",
    "    max_depth=7, \n",
    "    random_state=SEED, \n",
    "    silent=True, \n",
    "    metric='mse',\n",
    "    n_jobs=4, \n",
    "    n_estimators=200,\n",
    "    colsample_bytree=1,\n",
    "    subsample=0.9,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "META_MODEL.fit(x_train, y_train)\n",
    "final_predictions = META_MODEL.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "433299c0a6ef7fa1dabdb443310bde6b781d19168ce6c46534a3abeb460e20e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
