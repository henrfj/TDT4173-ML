{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "# Specific tf libraries\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all() \n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "%run ../common_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LaureRF</th>\n",
       "      <td>0.20015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF2</th>\n",
       "      <td>0.34266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep</th>\n",
       "      <td>0.23278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>0.19968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB1</th>\n",
       "      <td>0.23450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB1</th>\n",
       "      <td>0.23787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN1</th>\n",
       "      <td>0.35042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RMSLE\n",
       "LaureRF  0.20015\n",
       "RF2      0.34266\n",
       "Deep     0.23278\n",
       "GB       0.19968\n",
       "CB1      0.23450\n",
       "XGB1     0.23787\n",
       "KNN1     0.35042"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All their kaggle scores\n",
    "d = {'LaureRF': 0.20015,\n",
    "     'RF2': 0.34266,\n",
    "     'Deep': 0.23278,\n",
    "     'GB': 0.19968,\n",
    "     'CB1': 0.23450,\n",
    "     'XGB1': 0.23787,\n",
    "     'KNN1' : 0.35042}\n",
    "\n",
    "acc = pd.DataFrame(\n",
    "    d,\n",
    "    index=[0]\n",
    ")\n",
    "acc = acc.T\n",
    "acc.columns = ['RMSLE']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaureRF = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/LaureRF.csv\")\n",
    "RF2 = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/RF2.csv\")\n",
    "Deep = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/Deep.csv\")\n",
    "GB = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/GB.csv\")\n",
    "CB1 = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/CB1.csv\")\n",
    "XGB1 = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/XGB1.csv\")\n",
    "KNN1 = pd.read_csv(\"../henrik-testsite/ensemble_predictions/csvs/KNN1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaureRF = LaureRF.sort_values(by=\"id\")\n",
    "RF2 = RF2.sort_values(by=\"id\")\n",
    "Deep = Deep.sort_values(by=\"id\")\n",
    "GB = GB.sort_values(by=\"id\")\n",
    "CB1 = CB1.sort_values(by=\"id\")\n",
    "XGB1 = XGB1.sort_values(by=\"id\")\n",
    "KNN1 = KNN1.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaureRF_prediction = LaureRF[\"price_prediction\"].to_numpy().T\n",
    "RF2_prediction = RF2[\"price_prediction\"].to_numpy().T\n",
    "Deep_prediction = Deep[\"price_prediction\"].to_numpy().T\n",
    "GB_prediction = GB[\"price_prediction\"].to_numpy().T\n",
    "CB1_prediction = CB1[\"price_prediction\"].to_numpy().T\n",
    "XGB1_prediction = XGB1[\"price_prediction\"].to_numpy().T\n",
    "KNN1_prediction = KNN1[\"price_prediction\"].to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28507345.00363846  9414770.61022208  6555240.28133837 ...\n",
      " 10478451.89613688  9934283.66581429  7785634.31308344]\n"
     ]
    }
   ],
   "source": [
    "avg_prediction = np.average(\n",
    "    [LaureRF_prediction,\n",
    "     RF2_prediction,\n",
    "     Deep_prediction,\n",
    "     GB_prediction,\n",
    "     CB1_prediction,\n",
    "     XGB1_prediction,\n",
    "     KNN1_prediction\n",
    "    ],\n",
    "    weights = 1 / acc['RMSLE'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "print(avg_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = avg_prediction\n",
    "# submission = pd.DataFrame()\n",
    "# submission['id'] = LaureRF['id']\n",
    "# submission['price_prediction'] = result\n",
    "# if len(submission['id']) != 9937:\n",
    "#     raise Exception(\"Not enough rows submitted!\")\n",
    "# submission.to_csv('bagging_2', index=False)\n",
    "\n",
    "# # kaggle_scores = [0.23450, 0.20502, 0.23278, 0.19968, 0.35042, 0.20015, 0.20159, 0.23787]\n",
    "# # csv_paths = [\"ensemble_predictions/csvs/CB1.csv\", \"ensemble_predictions/csvs/deep_king_5_5.csv\", \"ensemble_predictions/csvs/deep.csv\", \"ensemble_predictions/csvs/GB.csv\",\n",
    "# #              \"ensemble_predictions/csvs/KNN1.csv\", \"ensemble_predictions/csvs/LaureRF.csv\", \"ensemble_predictions/csvs/xgb_king_2.csv\", \"ensemble_predictions/csvs/XGB1.csv\"]\n",
    "# # submission_path = \"ensemble_predictions/bagging_2\"\n",
    "\n",
    "# # csv_bagging(kaggle_scores, csv_paths, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, metaData = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot encoding\n",
      "minMax\n",
      "\n",
      ".................................."
     ]
    }
   ],
   "source": [
    "# Mostly the ones correlated to price.\n",
    "features = [\"building_id\",\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(\n",
    "    features, numerical_features, train, test, outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "    add_R=\"True\", add_rel_height=\"True\",  add_spacious=True, droptable=droptable,\n",
    "    one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "model_params = ([18, 12, 6], tf.nn.relu,\n",
    "                     [False, False, False], 0.2, tf.keras.optimizers.Adam,\n",
    "                      rmsle_custom, ['mse', 'msle', tf.keras.metrics.Accuracy()], True)\n",
    "\n",
    "# NB! We cannot just compile ann model beforehand, will just keep training on same model.\n",
    "ann_oof_train, ann_oof_test, hists = get_oof_ann(model_params, train_labels, train_targets, test_labels)\n",
    "\n",
    "# ADVISOR PERFORMANCE\n",
    "for histories in hists:\n",
    "    hist = pd.DataFrame(histories.history)\n",
    "    hist['epoch'] = histories.epoch\n",
    "    print(hist[\"val_loss\"].tail(1))\n",
    "print(np.sum(ann_oof_train >= 0) / len(ann_oof_train))\n",
    "print(np.sum(ann_oof_test >= 0) / len(ann_oof_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minMax\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20264/3768219413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     one_hot_encode=False, cat_features=cat_features, drop_old=True) # FALSE!\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mxgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mxgb_oof_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_oof_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof_xgboost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgboost' is not defined"
     ]
    }
   ],
   "source": [
    "# ALL, and building ID!\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude']\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=False, cat_features=cat_features, drop_old=True) # FALSE!\n",
    "\n",
    "xgb_model = xgboost.XGBRegressor(max_depth=5, min_child_weight=12, gamma=0.15, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.1, reg_lambda=0.3, learning_rate=0.01, n_estimators=10000)\n",
    "\n",
    "xgb_oof_train, xgb_oof_test, scores = get_oof_xgboost(xgb_model, train_labels, np.log(train_targets), test_labels)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11924/2386402069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_area\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillNan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Add new features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madded_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minMax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36mfeature_engineering\u001b[1;34m(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;31m# Newly_built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actually_new\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;31m# Newly_built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actually_new\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5507\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5508\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5509\u001b[1;33m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5511\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "lgbm_model = lightgbm.LGBMRegressor(\n",
    "  num_iterations= 10000,\n",
    "  n_estimators= 152,\n",
    "  learning_rate= 0.05,\n",
    "  num_leaves= 40,\n",
    "  max_depth= 10,\n",
    "  min_data_in_leaf= 20,\n",
    "  bagging_fraction= 0.9,\n",
    "  bagging_freq= 5,\n",
    "  feature_fraction= 0.8,\n",
    "  random_state=1,\n",
    "  early_stopping_round=100,\n",
    "  silent=True,\n",
    "  metric='regression',\n",
    "  num_threads=4\n",
    ")\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test, scores = get_oof_lgbm(lgbm_model, train_labels, np.log(train_targets), test_labels)\n",
    "# Advisor performance\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"building_id\",\n",
    "            \"area_total\", \"latitude\", \"longitude\", \"floor\", \"stories\", # Numerical\n",
    "            \"district\", 'condition' # Categorical\n",
    "           ]\n",
    "\n",
    "numerical = [] # No need to scale for RF! https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package\n",
    "cat_features = []\n",
    "droptable = []\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"none\",\n",
    "                    add_R=False, add_rel_height=False, add_spacious=False, droptable=droptable,\n",
    "                    one_hot_encode=False, cat_features=cat_features, drop_old=False) \n",
    "\n",
    "rf_oof_train, rf_oof_test, scores = get_oof_rf(train_labels, train_targets, test_labels) # Use log() for RF?\n",
    "\n",
    "print(scores)\n",
    "print(\"==>\\t\",np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"building_id\", # For Grouping\n",
    "            \"area_total\", \"ceiling\", \"floor\", \"bathrooms_shared\", \"balconies\", \"stories\", \"rooms\",\"latitude\",\"longitude\", # Numerical\n",
    "            \"windows_street\", \"elevator_without\", # Bool\n",
    "            \"parking\", \"heating\", \"district\", \"condition\"  # Categorical\n",
    "           ]\n",
    "\n",
    "numerical_features = [\"area_total\", \"ceiling\", \"floor\", \"bathrooms_shared\", \"balconies\", \"stories\", \"rooms\"]\n",
    "categorical_features = [\"parking\", \"heating\", \"district\", \"condition\"]\n",
    "droptable=[\"latitude\",\"longitude\"]\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=True, cat_features=cat_features, drop_old=False) \n",
    "\n",
    "knn_oof_train, knn_oof_test, scores = get_oof_knn(train_labels, train_targets, test_labels)\n",
    "\n",
    "print(scores)\n",
    "print(\"==>\\t\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL, and building ID!\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude']\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=True, cat_features=cat_features, drop_old=True) \n",
    "\n",
    "xgb_king = xgboost.XGBRegressor(max_depth=5, min_child_weight=12, gamma=0.15, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.1, reg_lambda=0.3, learning_rate=0.01, n_estimators=10000)\n",
    "\n",
    "# TODO: shouldn't xgb and lgbm predict the log(price)?\n",
    "# NB! ann predicts some negative values! Maybe RELU on the output?\n",
    "train_labels[\"xgb_preds\"] = xgb_oof_train\n",
    "train_labels[\"lgbm_preds\"] = lgbm_oof_train\n",
    "train_labels[\"ann_preds\"] = np.log(ann_oof_train)\n",
    "train_labels[\"knn_preds\"] = np.log(knn_oof_train)\n",
    "train_labels[\"rf_preds\"] = np.log(rf_oof_train)\n",
    "\n",
    "test_labels[\"xgb_preds\"] = lgbm_oof_test\n",
    "test_labels[\"lgbm_preds\"] = xgb_oof_test\n",
    "test_labels[\"ann_preds\"] = np.log(ann_oof_test)\n",
    "test_labels[\"knn_preds\"] = np.log(knn_oof_test)\n",
    "test_labels[\"rf_preds\"] = np.log(rf_oof_test)\n",
    "\n",
    "# Drop all except the good stuff?\n",
    "train_labels.drop(train_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "test_labels.drop(test_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "\n",
    "scores, avg, best_model, best_index = XGB_groupKFold(5, xgb_king, train_labels, np.log(train_targets),\n",
    "    eval_metric='rmse')\n",
    "\n",
    "# Kings performance\n",
    "print(scores)\n",
    "print(\"==>\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly the ones correlated to price.\n",
    "features =           [\"building_id\",\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(\n",
    "    features, numerical_features, train, test, outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "    add_R=\"True\", add_rel_height=\"True\",  add_spacious=True, droptable=droptable,\n",
    "    one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "train_labels[\"xgb_preds\"] = np.exp(xgb_oof_train)\n",
    "train_labels[\"lgbm_preds\"] = np.exp(lgbm_oof_train)\n",
    "train_labels[\"ann_preds\"] = ann_oof_train\n",
    "train_labels[\"knn_preds\"] = knn_oof_train\n",
    "train_labels[\"rf_preds\"] = rf_oof_train\n",
    "\n",
    "test_labels[\"xgb_preds\"] = np.exp(lgbm_oof_test)\n",
    "test_labels[\"lgbm_preds\"] = np.exp(xgb_oof_test)\n",
    "test_labels[\"ann_preds\"] = ann_oof_test\n",
    "test_labels[\"knn_preds\"] = knn_oof_test\n",
    "test_labels[\"rf_preds\"] = rf_oof_test\n",
    "\n",
    "# Drop all except the good stuff?\n",
    "train_labels.drop(train_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "test_labels.drop(test_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "\n",
    "## TODO! Use a deeper net, with regularization maybe :D\n",
    "model_params = ([18, 18, 18], tf.nn.leaky_relu,\n",
    "                     [False, False, False], 0.2, 'adam',\n",
    "                      rmsle_custom, ['mse', 'msle', tf.keras.metrics.Accuracy()], True)\n",
    "\n",
    "ann_scores, models, best_model, hists = ANN_groupKFold(5, model_params, train_labels, train_targets)\n",
    "\n",
    "print(ann_scores)\n",
    "print(\"==>\\t\", np.average(ann_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradientboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (this is all)\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "all_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"] # All are non-ordinal\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "model_no_cv = GradientBoostingRegressor(\n",
    "    n_estimators = optimal_n_estimators*10,\n",
    "    max_depth = optimal_max_depth,\n",
    "    min_samples_split = optimal_min_samples_split,\n",
    "    min_samples_leaf = optimal_min_samples_leaf,\n",
    "    max_features = optimal_max_features,\n",
    "    subsample = optimal_subsample,\n",
    "    learning_rate = original_learning_rate / 10,\n",
    "    loss = 'squared_error',\n",
    "    criterion = 'squared_error',\n",
    "    verbose = 0,\n",
    "    warm_start = False,\n",
    "    random_state = random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfoqubfouqfoq\n",
    "#stop the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((\n",
    "    dt_oof_train,\n",
    "    rf_oof_train,\n",
    "    ada_oof_train,\n",
    "    gb_oof_train,\n",
    "    lgbm_oof_train,\n",
    "    cat_oof_train\n",
    "), axis=1)\n",
    "\n",
    "x_test = np.concatenate((\n",
    "    dt_oof_test,\n",
    "    rf_oof_test,\n",
    "    ada_oof_test,\n",
    "    gb_oof_test,\n",
    "    lgbm_oof_test,\n",
    "    cat_oof_test\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "META_MODEL = lgb.LGBMRegressor(\n",
    "    num_leaves=5,\n",
    "    max_depth=7, \n",
    "    random_state=SEED, \n",
    "    silent=True, \n",
    "    metric='mse',\n",
    "    n_jobs=4, \n",
    "    n_estimators=200,\n",
    "    colsample_bytree=1,\n",
    "    subsample=0.9,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "META_MODEL.fit(x_train, y_train)\n",
    "final_predictions = META_MODEL.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "433299c0a6ef7fa1dabdb443310bde6b781d19168ce6c46534a3abeb460e20e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
