{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "# Specific tf libraries\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all() \n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "%run ../common_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'EnsembleData/Bagging_of_stacking/'\n",
    "\n",
    "d = {\n",
    "    'bagging_1.csv': 0.18088,\n",
    "    'bagging_2.csv': 0.17922,\n",
    "    'bagging_3.csv': 0.17847,\n",
    "    'bagging_5.csv': 0.18122,\n",
    "    'bagging_6.csv': 0.18419,\n",
    "    'bagging_9.csv': 0.17463,\n",
    "    'bagging_10.csv': 0.17797,\n",
    "    'bagging_12.csv': 0.17623,\n",
    "    'LGBM5.3.csv': 0.17987,\n",
    "    'stacking1.0.csv': 0.18519,\n",
    "    'stacking2.0.csv': 0.18519,\n",
    "    'stacking4.0.csv': 0.18230,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bagging_1.csv</th>\n",
       "      <td>0.18088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_2.csv</th>\n",
       "      <td>0.17922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_3.csv</th>\n",
       "      <td>0.17847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_5.csv</th>\n",
       "      <td>0.18122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_6.csv</th>\n",
       "      <td>0.18419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_9.csv</th>\n",
       "      <td>0.17463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_10.csv</th>\n",
       "      <td>0.17797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_12.csv</th>\n",
       "      <td>0.17623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM5.3.csv</th>\n",
       "      <td>0.17987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacking1.0.csv</th>\n",
       "      <td>0.18519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacking2.0.csv</th>\n",
       "      <td>0.18519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacking4.0.csv</th>\n",
       "      <td>0.18230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   RMSLE\n",
       "bagging_1.csv    0.18088\n",
       "bagging_2.csv    0.17922\n",
       "bagging_3.csv    0.17847\n",
       "bagging_5.csv    0.18122\n",
       "bagging_6.csv    0.18419\n",
       "bagging_9.csv    0.17463\n",
       "bagging_10.csv   0.17797\n",
       "bagging_12.csv   0.17623\n",
       "LGBM5.3.csv      0.17987\n",
       "stacking1.0.csv  0.18519\n",
       "stacking2.0.csv  0.18519\n",
       "stacking4.0.csv  0.18230"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = pd.DataFrame(\n",
    "    d,\n",
    "    index=[0]\n",
    ")\n",
    "acc = acc.T\n",
    "acc.columns = ['RMSLE']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for p in d.keys():\n",
    "    predictions.append(pd.read_csv(main_path + p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([28573425.55771748,  9455341.09146512,  6210004.32241076, ...,\n",
      "       10906865.04828714,  9433771.27088097,  7039881.08232997]), array([27689428.61143477,  9857877.06523608,  6150646.36427699, ...,\n",
      "       10763428.88355336,  9516744.75242097,  6850526.1012619 ]), array([26285337.27354194,  9647362.17293325,  6218602.54717076, ...,\n",
      "       10630958.1453088 ,  9541861.89536589,  6755555.10490426]), array([27903943.18697061, 10153876.11359155,  6162837.18698958, ...,\n",
      "       10557173.61525116,  9360426.17938904,  6824866.34052495]), array([23942261.25786896,  8914138.19536451,  6299327.86570683, ...,\n",
      "       10737767.81483457,  9804506.2308183 ,  6655220.92692299]), array([27283552.01049089,  9581712.47023847,  6287897.10967064, ...,\n",
      "       10690125.03163794,  9265160.70416494,  6577079.90717485]), array([27256989.33109218,  9817475.17732353,  6150992.27872368, ...,\n",
      "       10570826.30637594,  9615216.72609969,  6835741.20599826]), array([29247063.62181474, 10012064.00777242,  6289672.45605002, ...,\n",
      "        9984950.49618762,  9340046.15159505,  6571141.33950135]), array([30165500.60543181, 10326685.8431542 ,  5948609.92743708, ...,\n",
      "       10390829.63974617,  9834794.66097736,  7075767.64063086]), array([32683387.18340239, 10556030.11288109,  6401461.77871492, ...,\n",
      "       10483982.86709269,  8902926.99821273,  6379886.35739049]), array([32683387.18340239, 10556030.11288109,  6401461.77871492, ...,\n",
      "       10483982.86709269,  8902926.99821273,  6379886.35739049]), array([34277967.75120808,  9957444.04708753,  6375383.36866547, ...,\n",
      "        9603105.66114661,  8976939.17454354,  6281072.33644447])]\n"
     ]
    }
   ],
   "source": [
    "price_predictions = []\n",
    "for p in predictions:\n",
    "    p = p.sort_values(by=\"id\")\n",
    "    price_predictions.append(p[\"price_prediction\"].to_numpy().T)\n",
    "print(price_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28918401.72187938  9894756.03623791  6238001.74999932 ...\n",
      " 10484285.34294221  9381838.35069165  6691188.14957885]\n"
     ]
    }
   ],
   "source": [
    "avg_prediction = np.average(\n",
    "    price_predictions,\n",
    "    weights = 1 / acc['RMSLE'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "print(avg_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = avg_prediction\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = predictions[0]['id']\n",
    "submission['price_prediction'] = result\n",
    "if len(submission['id']) != 9937:\n",
    "    raise Exception(\"Not enough rows submitted!\")\n",
    "submission.to_csv('bagging_of_stacking_1', index=False)\n",
    "\n",
    "# kaggle_scores = [0.23450, 0.20502, 0.23278, 0.19968, 0.35042, 0.20015, 0.20159, 0.23787]\n",
    "# csv_paths = [\"ensemble_predictions/csvs/CB1.csv\", \"ensemble_predictions/csvs/deep_king_5_5.csv\", \"ensemble_predictions/csvs/deep.csv\", \"ensemble_predictions/csvs/GB.csv\",\n",
    "#              \"ensemble_predictions/csvs/KNN1.csv\", \"ensemble_predictions/csvs/LaureRF.csv\", \"ensemble_predictions/csvs/xgb_king_2.csv\", \"ensemble_predictions/csvs/XGB1.csv\"]\n",
    "# submission_path = \"ensemble_predictions/bagging_2\"\n",
    "\n",
    "# csv_bagging(kaggle_scores, csv_paths, submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop between bagging and stacking\n",
    "jdbosudgbpgibsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, metaData = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot encoding\n",
      "minMax\n",
      "\n",
      ".................................."
     ]
    }
   ],
   "source": [
    "# Mostly the ones correlated to price.\n",
    "features = [\"building_id\",\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(\n",
    "    features, numerical_features, train, test, outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "    add_R=\"True\", add_rel_height=\"True\",  add_spacious=True, droptable=droptable,\n",
    "    one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "model_params = ([18, 12, 6], tf.nn.relu,\n",
    "                     [False, False, False], 0.2, tf.keras.optimizers.Adam,\n",
    "                      rmsle_custom, ['mse', 'msle', tf.keras.metrics.Accuracy()], True)\n",
    "\n",
    "# NB! We cannot just compile ann model beforehand, will just keep training on same model.\n",
    "ann_oof_train, ann_oof_test, hists = get_oof_ann(model_params, train_labels, train_targets, test_labels)\n",
    "\n",
    "# ADVISOR PERFORMANCE\n",
    "for histories in hists:\n",
    "    hist = pd.DataFrame(histories.history)\n",
    "    hist['epoch'] = histories.epoch\n",
    "    print(hist[\"val_loss\"].tail(1))\n",
    "print(np.sum(ann_oof_train >= 0) / len(ann_oof_train))\n",
    "print(np.sum(ann_oof_test >= 0) / len(ann_oof_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minMax\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20264/3768219413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     one_hot_encode=False, cat_features=cat_features, drop_old=True) # FALSE!\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mxgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mxgb_oof_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_oof_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof_xgboost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgboost' is not defined"
     ]
    }
   ],
   "source": [
    "# ALL, and building ID!\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude']\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=False, cat_features=cat_features, drop_old=True) # FALSE!\n",
    "\n",
    "xgb_model = xgboost.XGBRegressor(max_depth=5, min_child_weight=12, gamma=0.15, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.1, reg_lambda=0.3, learning_rate=0.01, n_estimators=10000)\n",
    "\n",
    "xgb_oof_train, xgb_oof_test, scores = get_oof_xgboost(xgb_model, train_labels, np.log(train_targets), test_labels)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11924/2386402069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_area\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillNan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Add new features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madded_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_numerical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minMax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36mfeature_engineering\u001b[1;34m(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;31m# Newly_built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actually_new\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\common_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;31m# Newly_built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actually_new\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constructed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Laure\\Documents\\NTNU\\ML\\Project\\TDT4173-ML\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5507\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5508\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5509\u001b[1;33m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5511\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "lgbm_model = lightgbm.LGBMRegressor(\n",
    "  num_iterations= 10000,\n",
    "  n_estimators= 152,\n",
    "  learning_rate= 0.05,\n",
    "  num_leaves= 40,\n",
    "  max_depth= 10,\n",
    "  min_data_in_leaf= 20,\n",
    "  bagging_fraction= 0.9,\n",
    "  bagging_freq= 5,\n",
    "  feature_fraction= 0.8,\n",
    "  random_state=1,\n",
    "  early_stopping_round=100,\n",
    "  silent=True,\n",
    "  metric='regression',\n",
    "  num_threads=4\n",
    ")\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test, scores = get_oof_lgbm(lgbm_model, train_labels, np.log(train_targets), test_labels)\n",
    "# Advisor performance\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"building_id\",\n",
    "            \"area_total\", \"latitude\", \"longitude\", \"floor\", \"stories\", # Numerical\n",
    "            \"district\", 'condition' # Categorical\n",
    "           ]\n",
    "\n",
    "numerical = [] # No need to scale for RF! https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package\n",
    "cat_features = []\n",
    "droptable = []\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"none\",\n",
    "                    add_R=False, add_rel_height=False, add_spacious=False, droptable=droptable,\n",
    "                    one_hot_encode=False, cat_features=cat_features, drop_old=False) \n",
    "\n",
    "rf_oof_train, rf_oof_test, scores = get_oof_rf(train_labels, train_targets, test_labels) # Use log() for RF?\n",
    "\n",
    "print(scores)\n",
    "print(\"==>\\t\",np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"building_id\", # For Grouping\n",
    "            \"area_total\", \"ceiling\", \"floor\", \"bathrooms_shared\", \"balconies\", \"stories\", \"rooms\",\"latitude\",\"longitude\", # Numerical\n",
    "            \"windows_street\", \"elevator_without\", # Bool\n",
    "            \"parking\", \"heating\", \"district\", \"condition\"  # Categorical\n",
    "           ]\n",
    "\n",
    "numerical_features = [\"area_total\", \"ceiling\", \"floor\", \"bathrooms_shared\", \"balconies\", \"stories\", \"rooms\"]\n",
    "categorical_features = [\"parking\", \"heating\", \"district\", \"condition\"]\n",
    "droptable=[\"latitude\",\"longitude\"]\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=True, cat_features=cat_features, drop_old=False) \n",
    "\n",
    "knn_oof_train, knn_oof_test, scores = get_oof_knn(train_labels, train_targets, test_labels)\n",
    "\n",
    "print(scores)\n",
    "print(\"==>\\t\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL, and building ID!\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] #String\n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude']\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(features, numerical_features, train, test,\n",
    "                    outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "                    add_R=True, add_rel_height=True, add_spacious=True, droptable=droptable,\n",
    "                    one_hot_encode=True, cat_features=cat_features, drop_old=True) \n",
    "\n",
    "xgb_king = xgboost.XGBRegressor(max_depth=5, min_child_weight=12, gamma=0.15, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.1, reg_lambda=0.3, learning_rate=0.01, n_estimators=10000)\n",
    "\n",
    "# TODO: shouldn't xgb and lgbm predict the log(price)?\n",
    "# NB! ann predicts some negative values! Maybe RELU on the output?\n",
    "train_labels[\"xgb_preds\"] = xgb_oof_train\n",
    "train_labels[\"lgbm_preds\"] = lgbm_oof_train\n",
    "train_labels[\"ann_preds\"] = np.log(ann_oof_train)\n",
    "train_labels[\"knn_preds\"] = np.log(knn_oof_train)\n",
    "train_labels[\"rf_preds\"] = np.log(rf_oof_train)\n",
    "\n",
    "test_labels[\"xgb_preds\"] = lgbm_oof_test\n",
    "test_labels[\"lgbm_preds\"] = xgb_oof_test\n",
    "test_labels[\"ann_preds\"] = np.log(ann_oof_test)\n",
    "test_labels[\"knn_preds\"] = np.log(knn_oof_test)\n",
    "test_labels[\"rf_preds\"] = np.log(rf_oof_test)\n",
    "\n",
    "# Drop all except the good stuff?\n",
    "train_labels.drop(train_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "test_labels.drop(test_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "\n",
    "scores, avg, best_model, best_index = XGB_groupKFold(5, xgb_king, train_labels, np.log(train_targets),\n",
    "    eval_metric='rmse')\n",
    "\n",
    "# Kings performance\n",
    "print(scores)\n",
    "print(\"==>\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly the ones correlated to price.\n",
    "features =           [\"building_id\",\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                    \"district\", \"material\", \"parking\"] \n",
    "\n",
    "numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\",\n",
    "                      \"floor\", \"stories\", \"rooms\", \"bathrooms_shared\", \"balconies\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "cat_features = [\"district\", \"material\", \"parking\"]\n",
    "\n",
    "droptable = ['longitude', 'latitude', 'area_kitchen', 'area_living', 'floor', 'stories'] # Not dropping theta!\n",
    "\n",
    "train_labels, train_targets, test_labels = pre_process_numerical(\n",
    "    features, numerical_features, train, test, outliers_value=7, val_data=False, val_split=0.1, random_state=42, scaler=\"minMax\",\n",
    "    add_R=\"True\", add_rel_height=\"True\",  add_spacious=True, droptable=droptable,\n",
    "    one_hot_encode=True, cat_features=cat_features, drop_old=True)\n",
    "\n",
    "train_labels[\"xgb_preds\"] = np.exp(xgb_oof_train)\n",
    "train_labels[\"lgbm_preds\"] = np.exp(lgbm_oof_train)\n",
    "train_labels[\"ann_preds\"] = ann_oof_train\n",
    "train_labels[\"knn_preds\"] = knn_oof_train\n",
    "train_labels[\"rf_preds\"] = rf_oof_train\n",
    "\n",
    "test_labels[\"xgb_preds\"] = np.exp(lgbm_oof_test)\n",
    "test_labels[\"lgbm_preds\"] = np.exp(xgb_oof_test)\n",
    "test_labels[\"ann_preds\"] = ann_oof_test\n",
    "test_labels[\"knn_preds\"] = knn_oof_test\n",
    "test_labels[\"rf_preds\"] = rf_oof_test\n",
    "\n",
    "# Drop all except the good stuff?\n",
    "train_labels.drop(train_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "test_labels.drop(test_labels.columns.to_list()[1:-5], inplace=True, axis=1)\n",
    "\n",
    "## TODO! Use a deeper net, with regularization maybe :D\n",
    "model_params = ([18, 18, 18], tf.nn.leaky_relu,\n",
    "                     [False, False, False], 0.2, 'adam',\n",
    "                      rmsle_custom, ['mse', 'msle', tf.keras.metrics.Accuracy()], True)\n",
    "\n",
    "ann_scores, models, best_model, hists = ANN_groupKFold(5, model_params, train_labels, train_targets)\n",
    "\n",
    "print(ann_scores)\n",
    "print(\"==>\\t\", np.average(ann_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradientboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (this is all)\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\"] # Bool\n",
    "                     #\"street\", \"address\"] # Strings\n",
    "\n",
    "all_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"] # All are non-ordinal\n",
    "\n",
    "droptable = []\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "# Add new features\n",
    "train_labels, test_labels, added_features = feature_engineering(train_labels, test_labels, float_numerical_features, int_numerical_features, cat_features)\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "# One-hot encoding\n",
    "train_labels, test_labels = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "# Drop some features\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "model_no_cv = GradientBoostingRegressor(\n",
    "    n_estimators = optimal_n_estimators*10,\n",
    "    max_depth = optimal_max_depth,\n",
    "    min_samples_split = optimal_min_samples_split,\n",
    "    min_samples_leaf = optimal_min_samples_leaf,\n",
    "    max_features = optimal_max_features,\n",
    "    subsample = optimal_subsample,\n",
    "    learning_rate = original_learning_rate / 10,\n",
    "    loss = 'squared_error',\n",
    "    criterion = 'squared_error',\n",
    "    verbose = 0,\n",
    "    warm_start = False,\n",
    "    random_state = random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfoqubfouqfoq\n",
    "#stop the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((\n",
    "    dt_oof_train,\n",
    "    rf_oof_train,\n",
    "    ada_oof_train,\n",
    "    gb_oof_train,\n",
    "    lgbm_oof_train,\n",
    "    cat_oof_train\n",
    "), axis=1)\n",
    "\n",
    "x_test = np.concatenate((\n",
    "    dt_oof_test,\n",
    "    rf_oof_test,\n",
    "    ada_oof_test,\n",
    "    gb_oof_test,\n",
    "    lgbm_oof_test,\n",
    "    cat_oof_test\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "META_MODEL = lgb.LGBMRegressor(\n",
    "    num_leaves=5,\n",
    "    max_depth=7, \n",
    "    random_state=SEED, \n",
    "    silent=True, \n",
    "    metric='mse',\n",
    "    n_jobs=4, \n",
    "    n_estimators=200,\n",
    "    colsample_bytree=1,\n",
    "    subsample=0.9,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "META_MODEL.fit(x_train, y_train)\n",
    "final_predictions = META_MODEL.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "433299c0a6ef7fa1dabdb443310bde6b781d19168ce6c46534a3abeb460e20e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
