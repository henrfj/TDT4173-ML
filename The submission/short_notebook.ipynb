{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a2c537-ee36-4985-9440-743e7841ab1f",
   "metadata": {},
   "source": [
    "# Short notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e9276-e0bc-44a9-8224-67ee962f7357",
   "metadata": {},
   "source": [
    "**Kaggle competition**: *Moscow Housing*\n",
    "\n",
    "**Kaggle team name**: *Team 16*\n",
    "\n",
    "**Team members**: Name - studentId\n",
    "- Laure Beringer - \n",
    "- Hasse Rombouts - 566536\n",
    "- Henrik Fjellheim - 490763\n",
    "\n",
    "**Submission 1**: Bagging_copy_2.csv with public score 0.15661\n",
    "\n",
    "**Submission 2**: LGBM_2_copy_2.csv with public score 0.15653"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b168103-ea16-4d2d-8f60-03fa4d322833",
   "metadata": {},
   "source": [
    "## Short about the final submission\n",
    "\n",
    "The final submission is a \"*bagging*\" prediction, combining several submissions using a weighted average. We tried bagging with a lot of different models, both stand-alone models, and stacked meta-models. The models were trained individually on different aspects of the dataset, and used to create \"Out-of-fold\" (oof) datasets to train a meta-models; the *stacking* process, and to combine model predictions using a weighted average function; the *bagging* process. \n",
    "\n",
    "The advantage of having a wide variaty of models, training on different targets and with different labels, was that their final predictions would be biased differently - one model would not result in the same errors as the next. Using bagging and stacking, meta-models and meta-predictions can take advantage of this diversity, producing better results than any of the individual models could do by themselves.\n",
    "\n",
    "The final and best submission however ended up needing only three, relatively simple but state of the art, models in the bag. \n",
    "\n",
    "Note that during the submissions, something went wrong whith using the random state and reproducibility. We were thus unable to reproduce our absolute best solution (Bagging.csv with public score 0.15610) which is why we did not submit this on Kaggle. This bagging submission used the files: LGMB2.csv + CatBoost.csv + GradientBoost.csv (GB.csv on Kaggle), so we also included them in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7caa50-ec8f-4249-97c0-1fcd5a1da72c",
   "metadata": {},
   "source": [
    "## What is in the bag?\n",
    "\n",
    "1. **LGBM_2_copy_2.csv**\n",
    "2. **Catboost_copy_2.csv**\n",
    "3. **GradientBoost_copy_2.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69886cc-02ca-4596-aea5-950bd669e034",
   "metadata": {},
   "source": [
    "## Reproducing the Submission\n",
    "To reproduce this submission the following steps must be taken:\n",
    "* Ready the data\n",
    "    > - Import the data (note that the data needs to be stored one level above this notebook in a data folder, for example: `../data/apartments_train.csv`)\n",
    "    > - Clean the data\n",
    "    > - Add new features\n",
    "    > - Normalize the data\n",
    "    > - Dependent on the model: perform one-hot-encoding\n",
    "    > - Drop unnecessary features\n",
    "* Declare the models with optimal parameters\n",
    "* Train each models\n",
    "* Predict the testing data using each model\n",
    "* Bag the result\n",
    "    > - Load all .csv files\n",
    "    > - Use weighted averages to determine the importance of each model prediction in the final result\n",
    "    > - Store final result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f4017-8471-4a5e-93a3-65183eead5b5",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c024e0-beef-4db6-aae5-8925dffc76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af00b2-9462-4ada-b02b-bcbed844362c",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6a28b3-0f06-4af0-aaa8-6e3d2570d527",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_data(fraction_of_data=1, apartment_id='apartment_id',path=None):\n",
    "    \n",
    "    if path is not None:\n",
    "        # Metadata\n",
    "        metaData_apartment = pd.read_json(path+'../data/apartments_meta.json')\n",
    "        metaData_building = pd.read_json(path+'../data/buildings_meta.json')\n",
    "        metaData = pd.concat([metaData_apartment, metaData_building])\n",
    "\n",
    "        # Train\n",
    "        train_apartment = pd.read_csv(path+'../data/apartments_train.csv')\n",
    "        train_building = pd.read_csv(path+'../data/buildings_train.csv')\n",
    "\n",
    "        # Test\n",
    "        test_apartment = pd.read_csv(path+'../data/apartments_test.csv')\n",
    "        test_building = pd.read_csv(path+'../data/buildings_test.csv')\n",
    "    \n",
    "    else:\n",
    "    # Metadata\n",
    "        metaData_apartment = pd.read_json('../data/apartments_meta.json')\n",
    "        metaData_building = pd.read_json('../data/buildings_meta.json')\n",
    "        metaData = pd.concat([metaData_apartment, metaData_building])\n",
    "\n",
    "        # Train\n",
    "        train_apartment = pd.read_csv('../data/apartments_train.csv')\n",
    "        train_building = pd.read_csv('../data/buildings_train.csv')\n",
    "\n",
    "        # Test\n",
    "        test_apartment = pd.read_csv('../data/apartments_test.csv')\n",
    "        test_building = pd.read_csv('../data/buildings_test.csv')\n",
    "  \n",
    "    train = pd.merge(train_apartment, train_building, left_on='building_id', right_on='id')\n",
    "    train.rename(columns={'id_x' : apartment_id}, inplace=True)\n",
    "    train.drop('id_y', axis=1, inplace=True)\n",
    "    train = train.head(int(train.shape[0] * fraction_of_data))\n",
    "\n",
    "\n",
    "    test = pd.merge(test_apartment, test_building, left_on='building_id', right_on='id')\n",
    "    test.rename(columns={'id_x' : apartment_id}, inplace=True)\n",
    "    test.drop('id_y', axis=1, inplace=True)\n",
    "\n",
    "    return train, test, metaData\n",
    "\n",
    "def clean_data(train, test,\n",
    "                 features, float_numerical_features, int_numerical_features, cat_features,\n",
    "                 log_targets=True, log_area=True, fillNan=True, log1_area=False, log1_targets=False):\n",
    "    '''Clean the data according to best knowledge so far...\n",
    "   \n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    # Extract\n",
    "    train_labels = train[features]\n",
    "    test_labels = test[features]\n",
    "    train_targets = train['price']\n",
    "\n",
    "    # Log targets\n",
    "    if log_targets:\n",
    "        train_targets = np.log(train_targets)\n",
    "    elif log1_targets:\n",
    "        print(\"log1p targets!\")\n",
    "        train_targets = np.log1p(train_targets)\n",
    "\n",
    "    ## ------------------------------------------------------------------------------------------------ ##\n",
    "    # TODO: Shoul we use thise at all?\n",
    "    # Fill nans using correlated features\n",
    "    train_labels = fillnaReg(train_labels, ['area_total'], 'area_living')\n",
    "    test_labels = fillnaReg(test_labels, ['area_total'], 'area_living')\n",
    "    # Area_kitchen\n",
    "    train_labels = fillnaReg(train_labels, ['area_total', 'area_living'], 'area_kitchen')\n",
    "    test_labels = fillnaReg(test_labels, ['area_total', 'area_living'], 'area_kitchen')\n",
    "    # ceiling\n",
    "    is_outlier = ((train_labels[\"ceiling\"] > 10) | (train_labels[\"ceiling\"] < 0.5))\n",
    "    outliers = train_labels.copy()[is_outlier]\n",
    "    inliers_index=[]\n",
    "    for index in train_labels.index:\n",
    "        if index not in outliers.index:\n",
    "            inliers_index.append(index)\n",
    "    train_labels.loc[outliers.index,['ceiling']] = train_labels.loc[inliers_index,['ceiling']].mean()\n",
    "\n",
    "    is_outlier = ((test_labels[\"ceiling\"] > 10) | (test_labels[\"ceiling\"] < 0.5))\n",
    "    outliers = test_labels.copy()[is_outlier]\n",
    "    for index in test_labels.index:\n",
    "        if index not in outliers.index:\n",
    "            inliers_index.append(index)\n",
    "    test_labels.loc[outliers.index,['ceiling']] = test_labels.loc[test_labels.index.intersection(inliers_index),['ceiling']].mean()\n",
    "\n",
    "    ## ------------------------------------------------------------------------------------------------ ##\n",
    "\n",
    "    # Remove zero area living\n",
    "    # TODO: why not > 0?\n",
    "    remove_zero = [row[\"area_living\"] if row[\"area_living\"] >= 1 else row[\"area_total\"]*(train_labels[\"area_living\"].mean() / train_labels[\"area_total\"].mean()) for _, row in train_labels.iterrows()] \n",
    "    train_labels[\"area_living\"] = remove_zero\n",
    "\n",
    "    remove_zero = [row[\"area_living\"] if row[\"area_living\"] >= 1 else row[\"area_total\"]*(test_labels[\"area_living\"].mean() / test_labels[\"area_total\"].mean()) for _, row in test_labels.iterrows()] \n",
    "    test_labels[\"area_living\"] = remove_zero\n",
    "\n",
    "    # Log the areas\n",
    "    fs = [\"area_total\", \"area_living\", \"area_kitchen\"]\n",
    "    if log_area:\n",
    "        for feature in fs:\n",
    "            # Logging\n",
    "            train_labels[feature] = np.log(train_labels[feature])\n",
    "            test_labels[feature] = np.log(test_labels[feature])\n",
    "    elif log1_area:\n",
    "        print(\"log1p area!!\")\n",
    "        for feature in fs:\n",
    "            # Logging\n",
    "            train_labels[feature] = np.log1p(train_labels[feature])\n",
    "            test_labels[feature] = np.log1p(test_labels[feature])\n",
    "\n",
    "\n",
    "    # remove upper Strip\n",
    "    remove_upper_stripe = [row[\"area_living\"] if row[\"area_living\"] < row[\"area_total\"] else row[\"area_total\"]*(train_labels[\"area_living\"].mean() / train_labels[\"area_total\"].mean()) for _, row in train_labels.iterrows()] \n",
    "    train_labels[\"area_living\"] = remove_upper_stripe\n",
    "\n",
    "    remove_upper_stripe = [row[\"area_living\"] if row[\"area_living\"] < row[\"area_total\"] else row[\"area_total\"]*(test_labels[\"area_living\"].mean() / test_labels[\"area_total\"].mean()) for _, row in test_labels.iterrows()] \n",
    "    test_labels[\"area_living\"] = remove_upper_stripe\n",
    "    ## ------------------------------------------------------------------------------------------------ ##\n",
    "\n",
    "\n",
    "    # Fillnans of the two lat/log.\n",
    "    # Insert median district\n",
    "    #unknown_index = test_labels[[\"district\", \"latitude\", \"longitude\"]][test_labels[\"latitude\"].isna()==True].index\n",
    "    #test_labels.loc[unknown_index,['district']] = test_labels[\"district\"].median()\n",
    "    ## Mean the long/lat\n",
    "    #test_labels[\"longitude\"] = test_labels[\"longitude\"].fillna(test_labels[\"longitude\"].mean())\n",
    "    #test_labels[\"latitude\"] = test_labels[\"latitude\"].fillna(test_labels[\"latitude\"].mean())\n",
    "\n",
    "    # Both nans in test-data is in same known street.\n",
    "    # (37.470959, 55.570540)\n",
    "    is_na = (test_labels[\"longitude\"].isna())\n",
    "    nas = test_labels.copy()[is_na]\n",
    "    test_labels.loc[nas.index,['longitude']] = 37.470959\n",
    "    test_labels.loc[nas.index,['latitude']] = 55.570540\n",
    "    \n",
    "\n",
    "    # Fix houses on the \"north pole\".\n",
    "    is_outlier = (test_labels[\"longitude\"] > 39) | (test_labels[\"longitude\"] < 35)\n",
    "    outliers = test_labels.copy()[is_outlier]\n",
    "    for index in outliers.index:\n",
    "        if test_labels.loc[index,['street']][0]==\"Бунинские Луга ЖК\":\n",
    "            test_labels.loc[index,['longitude']] = 37.482297\n",
    "            test_labels.loc[index,['latitude']] = 55.543673\n",
    "        elif test_labels.loc[index,['street']][0]==\"улица Центральная\":\n",
    "            test_labels.loc[index,['longitude']] = 37.640383\n",
    "            test_labels.loc[index,['latitude']] = 55.566131\n",
    "        else:\n",
    "            test_labels.loc[index,['longitude']] = train_labels[\"longitude\"].mean()\n",
    "            test_labels.loc[index,['latitude']] = train_labels[\"latitude\"].mean()\n",
    "            test_labels.loc[index,['street']] = \"Ленинский проспект\"\n",
    "\n",
    "    \n",
    "    # Fill districts using long/lat\n",
    "    district_centre_long = np.array(train_labels[[\"district\", \"longitude\"]].groupby(['district']).mean())\n",
    "    district_centre_lat = np.array(train_labels[[\"district\", \"latitude\"]].groupby(['district']).mean())\n",
    "    remove_nan_districts = [closest_district(row[\"latitude\"],\n",
    "     row[\"longitude\"], district_centre_long, district_centre_lat) if math.isnan(row[\"district\"]) else row[\"district\"] for _, row in train_labels.iterrows()] \n",
    "    train_labels[\"district\"] = remove_nan_districts\n",
    "    remove_nan_districts = [closest_district(row[\"latitude\"],\n",
    "     row[\"longitude\"], district_centre_long, district_centre_lat) if math.isnan(row[\"district\"]) else row[\"district\"] for _, row in test_labels.iterrows()] \n",
    "    test_labels[\"district\"] = remove_nan_districts\n",
    "\n",
    "    # Fix the seller with unknown category\n",
    "    unknown_category = len(list(train_labels[\"seller\"].unique())) - 1\n",
    "    train_labels[\"seller\"] = train_labels[\"seller\"].fillna(unknown_category)\n",
    "    test_labels[\"seller\"] = test_labels[\"seller\"].fillna(unknown_category)\n",
    "    ## --------------------------------------------------------------------------------------------------------------- ##\n",
    "    if fillNan:\n",
    "        # Taking care of the string features first... NB! no nans in string data.\n",
    "        # Fill rest with median or mean\n",
    "        # Float\n",
    "        train_labels[float_numerical_features] = train_labels[float_numerical_features].fillna(train_labels[float_numerical_features].mean())\n",
    "        # Int\n",
    "        train_labels[int_numerical_features] = train_labels[int_numerical_features].fillna(train_labels[int_numerical_features].median())\n",
    "        # Cat\n",
    "        train_labels[cat_features] = train_labels[cat_features].fillna(train_labels[cat_features].median())\n",
    "        # Bool (The rest)\n",
    "        train_labels = train_labels.fillna(train_labels.median()) # Boolean\n",
    "\n",
    "        # Float\n",
    "        test_labels[float_numerical_features] = test_labels[float_numerical_features].fillna(test_labels[float_numerical_features].mean())\n",
    "        # Int\n",
    "        test_labels[int_numerical_features] = test_labels[int_numerical_features].fillna(test_labels[int_numerical_features].median())\n",
    "        # Cat\n",
    "        test_labels[cat_features] = test_labels[cat_features].fillna(test_labels[cat_features].median())\n",
    "        # Bool (The rest)\n",
    "        test_labels = test_labels.fillna(test_labels.median()) # Boolean\n",
    "\n",
    "    if \"constructed\" in features:\n",
    "        # Is supposed to be integer.\n",
    "        train[\"constructed\"] = np.asarray(train[\"constructed\"]).astype(\"int\")\n",
    "        test[\"constructed\"] = np.asarray(test[\"constructed\"]).astype(\"int\")\n",
    "\n",
    "    return train_labels, train_targets, test_labels\n",
    "\n",
    "def feature_engineering(train_labels, test_labels,\n",
    "    add_base_features=True, \n",
    "    add_bool_features=True,\n",
    "    add_weak_features=False,\n",
    "    add_dist_to_metro=False,\n",
    "    add_close_to_uni=False,\n",
    "    add_dist_to_hospital=False,\n",
    "    add_floor_features=False,\n",
    "    add_street_info=False,\n",
    "    add_some_more_features=False,\n",
    "    add_district_information=False,\n",
    "    ):\n",
    "\n",
    "    if add_base_features:\n",
    "        added_features = []\n",
    "        # Add R and theta\n",
    "        train_labels, test_labels = polar_coordinates(train_labels, test_labels)\n",
    "        added_features.append(\"r\")\n",
    "        added_features.append(\"theta\")\n",
    "\n",
    "        # Add \"Spacious_rooms\": area per room\n",
    "        train_labels['spacious_rooms'] = train_labels['area_total'] / train_labels['rooms']\n",
    "        test_labels['spacious_rooms'] = test_labels['area_total'] / test_labels['rooms']\n",
    "        added_features.append('spacious_rooms')\n",
    "\n",
    "        # Newly_built\n",
    "        is_new = [1 if row[\"constructed\"] >= 2000 else 0 for _, row in train_labels.iterrows()] \n",
    "        train_labels[\"actually_new\"] = is_new\n",
    "        is_new = [1 if row[\"constructed\"] >= 2000 else 0 for _, row in test_labels.iterrows()] \n",
    "        test_labels[\"actually_new\"] = is_new\n",
    "        added_features.append(\"actually_new\")\n",
    "\n",
    "        train_labels[\"rel_living\"] = np.asarray(train_labels[\"area_living\"] / train_labels[\"area_total\"]).astype(\"float32\")\n",
    "        test_labels[\"rel_living\"] = np.asarray(test_labels[\"area_living\"] / test_labels[\"area_total\"]).astype(\"float32\")\n",
    "        added_features.append(\"rel_living\")\n",
    "            \n",
    "        train_labels[\"total_bathrooms\"] = np.asarray(train_labels[\"bathrooms_private\"] + train_labels[\"bathrooms_shared\"]).astype(\"int\")\n",
    "        test_labels[\"total_bathrooms\"] = np.asarray(test_labels[\"bathrooms_private\"] + test_labels[\"bathrooms_shared\"]).astype(\"int\")\n",
    "        added_features.append(\"total_bathrooms\")\n",
    "\n",
    "    if add_bool_features:\n",
    "        # Some boolean features\n",
    "        train_labels[\"multiple_balconies\"] = np.asarray((train_labels[\"balconies\"]>1)).astype(\"int\")\n",
    "        test_labels[\"multiple_balconies\"] = np.asarray((test_labels[\"balconies\"]>1)).astype(\"int\")\n",
    "        added_features.append(\"multiple_balconies\")\n",
    "\n",
    "        train_labels[\"multiple_loggias\"] = np.asarray((train_labels[\"loggias\"]>1)).astype(\"int\")\n",
    "        test_labels[\"multiple_loggias\"] = np.asarray((test_labels[\"loggias\"]>1)).astype(\"int\")\n",
    "        added_features.append(\"multiple_loggias\")\n",
    "\n",
    "        train_labels[\"both_windows\"] = np.asarray((train_labels[\"windows_court\"]==True) & (train_labels[\"windows_street\"]==True)).astype(\"int\")\n",
    "        test_labels[\"both_windows\"] = np.asarray((test_labels[\"windows_court\"]==True) & (test_labels[\"windows_street\"]==True)).astype(\"int\")\n",
    "        added_features.append(\"both_windows\")\n",
    "\n",
    "    if add_weak_features:\n",
    "        # Weakly correlated ones.\n",
    "        train_labels[\"rel_kitchen\"] = np.asarray(train_labels[\"area_kitchen\"] / train_labels[\"area_total\"]).astype(\"float32\")\n",
    "        test_labels[\"rel_kitchen\"] = np.asarray(test_labels[\"area_kitchen\"] / test_labels[\"area_total\"]).astype(\"float32\")\n",
    "        added_features.append(\"rel_kitchen\")\n",
    "\n",
    "        train_labels['rel_height'] = np.asarray(train_labels[\"floor\"] / train_labels[\"stories\"]).astype(\"float32\")\n",
    "        test_labels['rel_height'] = np.asarray(test_labels[\"floor\"] / test_labels[\"stories\"]).astype(\"float32\")\n",
    "        added_features.append('rel_height')\n",
    "\n",
    "    if add_dist_to_metro:\n",
    "        # Pre-calculated\n",
    "        dist_to_metro_train = np.loadtxt(\"./external_datasets/metro_distances_train.csv\")\n",
    "        dist_to_metro_test = np.loadtxt(\"./external_datasets/metro_distances_test.csv\")\n",
    "        # Meter variant\n",
    "        train_labels[\"dist_to_metro_m\"] = dist_to_metro_train * (2*np.pi*(6371000) / 360)\n",
    "        test_labels[\"dist_to_metro_m\"] = dist_to_metro_test * (2*np.pi*(6371000) / 360)\n",
    "        added_features.append('dist_to_metro_m')\n",
    "        # Walking distance\n",
    "        train_labels[\"metro_walking_distance\"] = np.asarray(train_labels[\"dist_to_metro_m\"]<train_labels[\"dist_to_metro_m\"].median()).astype(\"int\")\n",
    "        test_labels[\"metro_walking_distance\"] = np.asarray(test_labels[\"dist_to_metro_m\"]<test_labels[\"dist_to_metro_m\"].median()).astype(\"int\")\n",
    "        added_features.append('metro_walking_distance')\n",
    "    \n",
    "    if add_close_to_uni:\n",
    "        # Uni location\n",
    "        uni_location = (37.5286, 55.7039)\n",
    "        ## Distance to state university\n",
    "        dist_to_uni_train = np.zeros(len(train_labels))\n",
    "        for i, row_t in train_labels.iterrows():\n",
    "            apartment = (row_t[\"longitude\"], row_t[\"latitude\"])\n",
    "            dist_to_uni_train[i] = eucledian_distance(apartment, uni_location)\n",
    "        dist_to_uni_test = np.zeros(len(test_labels))\n",
    "        for i, row_t in test_labels.iterrows():\n",
    "            apartment = (row_t[\"longitude\"], row_t[\"latitude\"])\n",
    "            dist_to_uni_test[i] = eucledian_distance(apartment, uni_location)\n",
    "        # Meter edition\n",
    "        dist_to_uni_train_meters = dist_to_uni_train*(2*np.pi*(6371000) / 360)\n",
    "        dist_to_uni_test_meters = dist_to_uni_test*(2*np.pi*(6371000) / 360)\n",
    "        # Close to uni \n",
    "        train_labels[\"close_to_uni\"] = np.asarray((dist_to_uni_train_meters<2000)).astype(\"int\")\n",
    "        test_labels[\"close_to_uni\"] = np.asarray((dist_to_uni_test_meters<2000)).astype(\"int\")\n",
    "        added_features.append('close_to_uni')\n",
    "\n",
    "    if add_dist_to_hospital:\n",
    "        # Location of state hospital\n",
    "        hosp_location = (37.389167, 55.746389)\n",
    "        ## Distance to state university\n",
    "        dist_to_hosp_train = np.zeros(len(train_labels))\n",
    "        for i, row_t in train_labels.iterrows():\n",
    "            apartment = (row_t[\"longitude\"], row_t[\"latitude\"])\n",
    "            dist_to_hosp_train[i] = eucledian_distance(apartment, hosp_location)\n",
    "        dist_to_hosp_test = np.zeros(len(test_labels))\n",
    "        for i, row_t in test_labels.iterrows():\n",
    "            apartment = (row_t[\"longitude\"], row_t[\"latitude\"])\n",
    "            dist_to_hosp_test[i] = eucledian_distance(apartment, hosp_location)\n",
    "\n",
    "        dist_to_hosp_train_meters = dist_to_hosp_train*(2*np.pi*(6371000) / 360)\n",
    "        train_labels[\"dist_to_hospital_m\"] = dist_to_hosp_train_meters\n",
    "        dist_to_hosp_test_meters = dist_to_hosp_test*(2*np.pi*(6371000) / 360)\n",
    "        test_labels[\"dist_to_hospital_m\"] = dist_to_hosp_test_meters\n",
    "        added_features.append('dist_to_hospital_m')\n",
    "\n",
    "    if add_floor_features:\n",
    "        train_labels[\"lives_in_highrise\"] = np.asarray((train_labels[\"stories\"]>30)).astype(\"int\")\n",
    "        test_labels[\"lives_in_highrise\"] = np.asarray((test_labels[\"stories\"]>30)).astype(\"int\")\n",
    "        added_features.append('lives_in_highrise')\n",
    "\n",
    "        train_labels[\"first_floor\"] = np.asarray(train_labels[\"floor\"]==1).astype(\"int\")\n",
    "        test_labels[\"first_floor\"] = np.asarray(test_labels[\"floor\"]==1).astype(\"int\")\n",
    "        added_features.append('first_floor')\n",
    "        \n",
    "        train_labels[\"floor_inverse\"] = train_labels[\"stories\"]-train_labels[\"floor\"]\n",
    "        median_floor_inverse = np.median(train_labels[\"floor_inverse\"])\n",
    "        train_labels[\"floor_inverse\"]=train_labels[\"floor_inverse\"].where(train_labels[\"floor_inverse\"]>=0, other=median_floor_inverse)\n",
    "        test_labels[\"floor_inverse\"] = test_labels[\"stories\"]-test_labels[\"floor\"]\n",
    "        test_labels[\"floor_inverse\"]=test_labels[\"floor_inverse\"].where(test_labels[\"floor_inverse\"]>=0, other=median_floor_inverse)\n",
    "        added_features.append('floor_inverse')\n",
    "\n",
    "    if add_street_info:\n",
    "        # Seafront = набережная.\n",
    "        # This was pretty much the only one correlated to price.\n",
    "        on_type = [1 if \"набережная\" in row[\"street\"] else 0 for _, row in train_labels.iterrows()] \n",
    "        train_labels[\"on_seafront\"] = on_type\n",
    "        on_type = [1 if \"набережная\" in row[\"street\"] else 0 for _, row in test_labels.iterrows()] \n",
    "        test_labels[\"on_seafront\"] = on_type\n",
    "        added_features.append('on_seafront')\n",
    "        # Also, these streets seemed good to live in.\n",
    "        good_streets = [\"Мосфильмовская улица\", \"набережная Пресненская\",\n",
    "         \"улица Ефремова\", \"Казарменный переулок\"]\n",
    "        for street_name in good_streets:\n",
    "            on_street = [1 if street_name in row[\"street\"] else 0 for _, row in train_labels.iterrows()] \n",
    "            train_labels[\"in_\"+street_name] = on_street\n",
    "        for street_name in good_streets:\n",
    "            on_street = [1 if street_name in row[\"street\"] else 0 for _, row in test_labels.iterrows()] \n",
    "            test_labels[\"in_\"+street_name] = on_street\n",
    "            added_features.append(\"in_\"+street_name)\n",
    "        \n",
    "    if add_some_more_features:\n",
    "        # This is moved to data_cleaning.\n",
    "        #train_labels = train_labels.astype({'constructed':'int'})\n",
    "        #test_labels = test_labels.astype({'constructed':'int'})\n",
    "\n",
    "        train_labels[\"area_floor\"] = train_labels[\"area_total\"] / train_labels[\"floor\"]\n",
    "        train_labels[\"area_stories\"] = train_labels[\"area_total\"] / train_labels[\"stories\"]\n",
    "        train_labels[\"area_rooms\"] = train_labels[\"area_total\"] / np.average(train_labels[\"rooms\"]) # is just the invert of spacious rooms\n",
    "        train_labels[\"old_building\"] = (train_labels[\"constructed\"]<1950)\n",
    "        train_labels[\"cold_war_building\"] = (train_labels[\"constructed\"]>1955) & (train_labels[\"constructed\"]<2000)\n",
    "        train_labels[\"modern_but_not_too_modern\"] = (train_labels[\"constructed\"]>200) & (train_labels[\"constructed\"]<2018)\n",
    "        train_labels[\"bathroom_area\"] = (train_labels[\"bathrooms_private\"] + train_labels[\"bathrooms_shared\"])/train_labels[\"area_total\"]\n",
    "        train_labels['bathrooms_per_room'] = (train_labels[\"total_bathrooms\"])/train_labels[\"rooms\"]\n",
    "\n",
    "        test_labels[\"area_floor\"] = test_labels[\"area_total\"] / test_labels[\"floor\"]\n",
    "        test_labels[\"area_stories\"] = test_labels[\"area_total\"] / test_labels[\"stories\"]\n",
    "        test_labels[\"area_rooms\"] = test_labels[\"area_total\"] / np.average(test_labels[\"rooms\"]) # is just the invert of spacious rooms\n",
    "        test_labels[\"old_building\"] = (test_labels[\"constructed\"]<1950)\n",
    "        test_labels[\"cold_war_building\"] = (test_labels[\"constructed\"]>1955) & (test_labels[\"constructed\"]<2000)\n",
    "        test_labels[\"modern_but_not_too_modern\"] = (test_labels[\"constructed\"]>200) & (test_labels[\"constructed\"]<2018)\n",
    "        test_labels[\"bathroom_area\"] = (test_labels[\"bathrooms_private\"] + test_labels[\"bathrooms_shared\"])/test_labels[\"area_total\"]\n",
    "        test_labels['bathrooms_per_room'] = (test_labels[\"total_bathrooms\"])/test_labels[\"rooms\"]\n",
    "\n",
    "\n",
    "        # Calculate district average areas\n",
    "        arr_train = train_labels[[\"district\",\"area_total\",\"area_living\",\"area_kitchen\",]].to_numpy()\n",
    "        arr_test = test_labels[[\"district\",\"area_total\",\"area_living\",\"area_kitchen\",]].to_numpy()\n",
    "\n",
    "        arr_full = np.concatenate((arr_train,arr_test),axis=0)\n",
    "\n",
    "        average_area_total_district = {}\n",
    "        average_area_living_district = {}\n",
    "        average_area_kitchen_district = {}\n",
    "\n",
    "        for x in sorted(np.unique(arr_full[...,0])):\n",
    "            average_area_total_district[x] = np.average(arr_full[np.where(arr_full[...,0]==x)][...,1])\n",
    "            average_area_living_district[x] = np.average(arr_full[np.where(arr_full[...,0]==x)][...,2])\n",
    "            average_area_kitchen_district[x] = np.average(arr_full[np.where(arr_full[...,0]==x)][...,3])\n",
    "\n",
    "        # Calculate floor average areas\n",
    "        arr_train = train_labels[[\"floor\",\"area_total\"]].to_numpy()\n",
    "        arr_test = test_labels[[\"floor\",\"area_total\"]].to_numpy()\n",
    "\n",
    "        arr_full = np.concatenate((arr_train,arr_test),axis=0)\n",
    "\n",
    "        average_area_total_floor = {}\n",
    "        for x in sorted(np.unique(arr_full[...,0])):\n",
    "            average_area_total_floor[x] = np.average(arr_full[np.where(arr_full[...,0]==x)][...,1])\n",
    "   \n",
    "        # Calculate construction year average areas\n",
    "        arr_train = train_labels[[\"constructed\",\"area_total\",\"area_living\",\"area_kitchen\",]].to_numpy()\n",
    "        arr_test = test_labels[[\"constructed\",\"area_total\",\"area_living\",\"area_kitchen\",]].to_numpy()\n",
    "        \n",
    "        arr_full = np.concatenate((arr_train,arr_test),axis=0)\n",
    "\n",
    "        average_area_total_constructed = {}\n",
    "        for x in sorted(np.unique(arr_full[...,0])):\n",
    "            average_area_total_constructed[x] = np.average(arr_full[np.where(arr_full[...,0]==x)][...,1])\n",
    "   \n",
    "        total_district = []\n",
    "        living_district = []\n",
    "        kitchen_district = []\n",
    "\n",
    "        total_floor = []\n",
    "        total_constructed = []\n",
    "\n",
    "        for _,row in train_labels.iterrows():\n",
    "            total_district.append(row['area_total']/average_area_total_district[row['district']])\n",
    "            living_district.append(row['area_living']/average_area_living_district[row['district']])\n",
    "            kitchen_district.append(row['area_kitchen']/average_area_kitchen_district[row['district']])\n",
    "            total_floor.append(row['area_total']/average_area_total_floor[row['floor']])\n",
    "            total_constructed.append(row['area_total']/average_area_total_constructed[row['constructed']] if (pd.notnull(row['area_total']) and pd.notnull(row['constructed'])) else None)\n",
    "        \n",
    "        total_district_test = []\n",
    "        living_district_test = []\n",
    "        kitchen_district_test = []\n",
    "\n",
    "        total_floor_test = []\n",
    "        total_constructed_test = []\n",
    "\n",
    "        for _,row in test_labels.iterrows():\n",
    "            total_district_test.append(row['area_total']/average_area_total_district[row['district']])\n",
    "            living_district_test.append(row['area_living']/average_area_living_district[row['district']])\n",
    "            kitchen_district_test.append(row['area_kitchen']/average_area_kitchen_district[row['district']])\n",
    "            total_floor_test.append(row['area_total']/average_area_total_floor[row['floor']])\n",
    "            total_constructed_test.append(row['area_total']/average_area_total_constructed[row['constructed']] if (pd.notnull(row['area_total']) and pd.notnull(row['constructed'])) else None)\n",
    "\n",
    "        train_labels['average_area_total_district'] = total_district\n",
    "        train_labels['average_area_living_district'] = living_district\n",
    "        train_labels['average_area_kitchen_district'] = kitchen_district\n",
    "        train_labels['average_area_total_floor'] = total_floor\n",
    "        train_labels['average_area_year_constructed'] = total_constructed\n",
    "\n",
    "        test_labels['average_area_total_district'] = total_district_test\n",
    "        test_labels['average_area_living_district'] = living_district_test\n",
    "        test_labels['average_area_kitchen_district'] = kitchen_district_test\n",
    "        test_labels['average_area_total_floor'] = total_floor_test\n",
    "        test_labels['average_area_year_constructed'] = total_constructed_test\n",
    "    \n",
    "    if add_district_information:\n",
    "        density = {\n",
    "            0: 701353/66.1755,\n",
    "            1: 1112846/109.9,\n",
    "            2: 1240062/101.889,\n",
    "            3: 1394497/154.6,\n",
    "            4: 1116924/117.6,\n",
    "            5: 1593065/132,\n",
    "            6: 1179211/111.4,\n",
    "            7: 1049104/153,\n",
    "            8: 779965/93.281,\n",
    "            9: 215727/37.22,\n",
    "            10:86752/1084.3,\n",
    "            11:113569/361.4\n",
    "          }\n",
    "        population = {\n",
    "            0: 701353,\n",
    "            1: 1112846,\n",
    "            2: 1240062,\n",
    "            3: 1394497,\n",
    "            4: 1116924,\n",
    "            5: 1593065,\n",
    "            6: 1179211,\n",
    "            7: 1049104,\n",
    "            8: 779965,\n",
    "            9: 215727,\n",
    "            10:86752,\n",
    "            11:113569\n",
    "                }\n",
    "        district_area ={\n",
    "            0: 66.1755,\n",
    "            1: 109.9,\n",
    "            2: 101.889,\n",
    "            3: 154.6,\n",
    "            4: 117.6,\n",
    "            5: 132,\n",
    "            6: 111.4,\n",
    "            7: 153,\n",
    "            8: 93.281,\n",
    "            9: 37.22,\n",
    "            10:1084.3,\n",
    "            11:361.4\n",
    "                }\n",
    "\n",
    "        district_popularity_train_set = train_labels[['district','building_id']].groupby(['district']).count()\n",
    "        district_popularity_test_set = test_labels[['district','building_id']].groupby(['district']).count()\n",
    "        population_df = pd.DataFrame.from_dict(population,orient='index')\n",
    "        total = len(train_labels) + len(test_labels)\n",
    "        popularity_data_df = (district_popularity_train_set+district_popularity_test_set)/total\n",
    "        popularity_data = popularity_data_df.to_dict()['building_id']\n",
    "        popularity_population_df = pd.DataFrame.from_dict(popularity_data,orient='index')/population_df\n",
    "        popularity_population = popularity_population_df.to_dict()[0]\n",
    "\n",
    "\n",
    "\n",
    "        density_district = [density[row['district']] for _,row in train_labels.iterrows()]\n",
    "        population_district = [population[row['district']] for _,row in train_labels.iterrows()]\n",
    "        area_district = [district_area[row['district']] for _,row in train_labels.iterrows()]\n",
    "        outside_MKAD = [1 if (row[\"district\"] in [9,10,11]) else 0 for _,row in train_labels.iterrows()]\n",
    "        popularity_district_data = [popularity_data[row['district']] for _,row in train_labels.iterrows()]\n",
    "        popularity_district_population = [popularity_population[row['district']] for _,row in train_labels.iterrows()]\n",
    "\n",
    "        train_labels[\"density_district\"] = density_district\n",
    "        train_labels[\"population_district\"] = population_district\n",
    "        train_labels[\"area_district\"] = area_district\n",
    "        train_labels[\"outside_MKAD\"] = outside_MKAD        \n",
    "        train_labels[\"popularity_district_data\"] = popularity_district_data\n",
    "        train_labels[\"popularity_district_population\"] = popularity_district_population\n",
    "\n",
    "\n",
    "        density_district_test = [density[row['district']] for _,row in test_labels.iterrows()]\n",
    "        population_district_test = [population[row['district']] for _,row in test_labels.iterrows()]\n",
    "        area_district_test = [district_area[row['district']] for _,row in test_labels.iterrows()]\n",
    "        outside_MKAD_test = [1 if (row[\"district\"] in [9,10,11]) else 0 for _,row in test_labels.iterrows()]\n",
    "        popularity_district_data_test = [popularity_data[row['district']] for _,row in test_labels.iterrows()]\n",
    "        popularity_district_population_test = [popularity_population[row['district']] for _,row in test_labels.iterrows()]\n",
    "\n",
    "        test_labels[\"density_district\"] = density_district_test\n",
    "        test_labels[\"population_district\"] = population_district_test\n",
    "        test_labels[\"area_district\"] = area_district_test\n",
    "        test_labels[\"outside_MKAD\"] = outside_MKAD_test\n",
    "        test_labels[\"popularity_district_data\"] = popularity_district_data_test\n",
    "        test_labels[\"popularity_district_population\"] = popularity_district_population_test\n",
    "\n",
    "\n",
    "    return train_labels, test_labels, added_features\n",
    "\n",
    "def normalize(train_labels, test_labels, features, scaler=\"minMax\"):\n",
    "    # Only normalize/scale the numerical data. Categorical data is kept as is.\n",
    "    train_labels_n = train_labels.filter(features)\n",
    "    test_labels_n = test_labels.filter(features)\n",
    "\n",
    "    # Scale it.\n",
    "    if scaler==\"minMax\":\n",
    "        print(\"minMax\")\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        train_labels_scaled = scaler.fit_transform(train_labels_n)\n",
    "        test_labels_scaled = scaler.transform(test_labels_n)\n",
    "    elif scaler==\"std\":\n",
    "        print(\"Std\")\n",
    "        std_scale = preprocessing.StandardScaler().fit(train_labels_n)\n",
    "        train_labels_scaled = std_scale.transform(train_labels_n)\n",
    "        test_labels_scaled = std_scale.transform(test_labels_n)\n",
    "    else:\n",
    "        assert ValueError, \"Incorrect scaler\"\n",
    "\n",
    "    # Re-enter proceedure\n",
    "    training_norm_col = pd.DataFrame(train_labels_scaled, index=train_labels_n.index, columns=train_labels_n.columns) \n",
    "    train_labels.update(training_norm_col)\n",
    "\n",
    "    testing_norm_col = pd.DataFrame(test_labels_scaled, index=test_labels_n.index, columns=test_labels_n.columns) \n",
    "    test_labels.update(testing_norm_col)\n",
    "\n",
    "    return train_labels, test_labels\n",
    "\n",
    "\n",
    "def one_hot_encoder(train_df, test_df, cat_features, drop_old=True):\n",
    "    '''\n",
    "    Returns a copy of all three dfs, after one-hot encoding and !removing!\n",
    "    their old cat_features.\n",
    "\n",
    "    NB! pd.get_dummies() does pretty much the same job!\n",
    "    https://stackoverflow.com/questions/36285155/pandas-get-dummies\n",
    "    BUG! Some categories are only present in train not test or the other way around!\n",
    "        - Then the encoding is made differently for the two!\n",
    "    https://stackoverflow.com/questions/57946006/one-hot-encoding-train-with-values-not-present-on-test\n",
    "    '''\n",
    "    # TODO: use Laures one-hot encoder.\n",
    "    \n",
    "    #if(len(train_df.isna())!=0 or len(train_df.isna())!=0 or len(train_df.isna())!=0):\n",
    "    #    assert ValueError\n",
    "\n",
    "    train_labels = train_df.copy()\n",
    "    test_labels = test_df.copy()\n",
    "\n",
    "    encoded_features = []\n",
    "    dfs=[train_labels, test_labels]\n",
    "    for df in dfs:\n",
    "        for feature in cat_features:\n",
    "            encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
    "            n = df[feature].nunique()\n",
    "            cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "            encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "            encoded_df.index = df.index\n",
    "            encoded_features.append(encoded_df)\n",
    "\n",
    "    n = len(cat_features)\n",
    "\n",
    "    train_labels = pd.concat([train_labels, *encoded_features[ : n]], axis=1)\n",
    "    test_labels = pd.concat([test_labels, *encoded_features[n : ]], axis=1)\n",
    "\n",
    "\n",
    "    # Now drop the non-encoded ones!\n",
    "    if drop_old:\n",
    "        train_labels.drop(cat_features, inplace=True, axis=1)\n",
    "        test_labels.drop(cat_features, inplace=True, axis=1)\n",
    "    return train_labels, test_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f39763-3de7-46c0-82c7-dac85a393c06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fillnaReg(df, X_features, y_feature):\n",
    "    df = df.copy()\n",
    "    df_temp = df[df[y_feature].notna()]\n",
    "    if df_temp.shape[0] == 1: df_temp = df_temp.values.reshape(-1, 1)\n",
    "    reg = LinearRegression().fit(df_temp[X_features], df_temp[y_feature])\n",
    "    predict = reg.predict(df[X_features])\n",
    "    df[y_feature] = np.where(df[y_feature]>0, df[y_feature], predict)\n",
    "    return df\n",
    "\n",
    "def fillnaReg(df, X_features, y_feature):\n",
    "    df = df.copy()\n",
    "    df_temp = df[df[y_feature].notna()]\n",
    "    if df_temp.shape[0] == 1: df_temp = df_temp.values.reshape(-1, 1)\n",
    "    reg = LinearRegression().fit(df_temp[X_features], df_temp[y_feature])\n",
    "    predict = reg.predict(df[X_features])\n",
    "    df[y_feature] = np.where(df[y_feature]>0, df[y_feature], predict)\n",
    "    return df\n",
    "\n",
    "def closest_district(lat, long, district_centre_long, district_centre_lat):\n",
    "    best_distance = -1\n",
    "    closest_dist = -1\n",
    "    for i in range(len(district_centre_long)):\n",
    "        long_dist =  district_centre_long[i][0]\n",
    "        lat_dist = district_centre_lat[i][0]\n",
    "        total_dist = np.sqrt((long-long_dist)**2 + (lat-lat_dist)**2)\n",
    "        if total_dist < best_distance or best_distance==-1:\n",
    "            best_distance = total_dist\n",
    "            closest_dist = i\n",
    "    return closest_dist\n",
    "\n",
    "def polar_coordinates(train_labels, test_labels, use_city_centre=False):\n",
    "    '''\n",
    "    labels and input labels. Adds theta and also R to the dataframe copy.\n",
    "    '''\n",
    "    # Make a copy to manipulate\n",
    "    labels1_normed_r = train_labels[[\"latitude\", \"longitude\"]].copy()\n",
    "    test1_normed_r = test_labels[[\"latitude\", \"longitude\"]].copy()\n",
    "\n",
    "    # City centre 55.755833, 37.617222 (lat/long)\n",
    "    # Instead of mean, use city centre: \n",
    "    if use_city_centre:\n",
    "        labels1_normed_r['latitude'] = labels1_normed_r['latitude'] - 55.755833\n",
    "        labels1_normed_r['longitude'] = labels1_normed_r['longitude'] - 37.617222\n",
    "        test1_normed_r['latitude'] = test1_normed_r['latitude'] -  55.755833\n",
    "        test1_normed_r['longitude'] = test1_normed_r['longitude'] -  37.617222\n",
    "    else: # just use means\n",
    "        # FLOAT\n",
    "        train_float_mean = train_labels[\"longitude\"].mean()\n",
    "        test_float_mean = test_labels[\"longitude\"].mean()\n",
    "        tl = len(train_labels) + len(test_labels)\n",
    "        total_mean_long = (train_float_mean*len(train_labels) + test_float_mean*len(test_labels)) / tl\n",
    "\n",
    "        train_float_mean = train_labels[\"latitude\"].mean()\n",
    "        test_float_mean = test_labels[\"latitude\"].mean()\n",
    "        tl = len(train_labels) + len(test_labels)\n",
    "        total_mean_lat = (train_float_mean*len(train_labels) + test_float_mean*len(test_labels)) / tl \n",
    "\n",
    "        labels1_normed_r['latitude'] = labels1_normed_r['latitude'] - total_mean_lat\n",
    "        labels1_normed_r['longitude'] = labels1_normed_r['longitude'] - total_mean_long\n",
    "        test1_normed_r['latitude'] = test1_normed_r['latitude'] -  total_mean_lat\n",
    "        test1_normed_r['longitude'] = test1_normed_r['longitude'] -  total_mean_long\n",
    "\n",
    "    # Convert to polar coordinates\n",
    "    labels1_normed_r['r'] =  np.sqrt(labels1_normed_r['latitude']**2 + labels1_normed_r['longitude']**2)\n",
    "    labels1_normed_r['theta'] = np.arctan(labels1_normed_r['longitude']/labels1_normed_r['latitude'])\n",
    "    test1_normed_r['r'] =  np.sqrt(test1_normed_r['latitude']**2 + test1_normed_r['longitude']**2)\n",
    "    test1_normed_r['theta'] = np.arctan(test1_normed_r['longitude']/test1_normed_r['latitude'])\n",
    "\n",
    "    # Add polar coordinates\n",
    "    train_labels[\"r\"] = labels1_normed_r['r']\n",
    "    train_labels[\"theta\"] = labels1_normed_r['theta']\n",
    "    test_labels[\"r\"] = test1_normed_r['r']\n",
    "    test_labels[\"theta\"] = test1_normed_r['theta']\n",
    "\n",
    "    return train_labels, test_labels\n",
    "\n",
    "\n",
    "def eucledian_distance(point1, point2):\n",
    "    return np.sqrt((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ea53d4-adbc-4301-9271-5f2bd8c5458a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_and_store(model, test_labels, test_pd, path=\"default\", exponential=False, price_per_sq = False, total_area_df = None, exponentialm1=False):\n",
    "    '''\n",
    "        Inputs\n",
    "        - test_pd needs to be the original full test dataframe\n",
    "    '''\n",
    "    result = model.predict(test_labels)\n",
    "    if exponential:\n",
    "        result = np.exp(result)\n",
    "    elif exponentialm1:\n",
    "        print(\"expm1\")\n",
    "        result = np.expm1(result)\n",
    "    if price_per_sq:\n",
    "        result = result*total_area_df\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = test_pd['apartment_id']\n",
    "    submission['price_prediction'] = result\n",
    "    if len(submission['id']) != 9937:\n",
    "        raise Exception(\"Not enough rows submitted!\")\n",
    "    submission.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a3f27-89dd-44dd-b97f-54a6ec076b0f",
   "metadata": {},
   "source": [
    "#### Obtain Final Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b2cbc7-beaa-4705-bcea-8fdd966b27cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minMax\n",
      "You have now imported all data!\n"
     ]
    }
   ],
   "source": [
    "# Declare features\n",
    "features =           [\"building_id\", # For grouping\n",
    "                      \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"ceiling\", \"stories\", \"rooms\",\n",
    "                      \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\", # Numerical\n",
    "                     \"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\", #Categorical\n",
    "                      \"windows_court\", \"windows_street\", \"new\", \"elevator_without\", \"elevator_passenger\", \"elevator_service\", \"garbage_chute\", # Bool\n",
    "                     \"street\", \"address\"] # Strings\n",
    "\n",
    "all_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\n",
    "                      \"ceiling\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\",\"loggias\", \"phones\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "\n",
    "float_numerical_features = [\"area_total\", \"area_kitchen\", \"area_living\", \"ceiling\", \"latitude\", \"longitude\", \"constructed\"]\n",
    "int_numerical_features = [\"floor\", \"stories\", \"rooms\", \"bathrooms_private\", \"bathrooms_shared\", \"balconies\", \"loggias\", \"phones\"] # Ordinal categories\n",
    "\n",
    "cat_features = [\"layout\", \"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"] # All are non-ordinal\n",
    "\n",
    "# Load data\n",
    "train, test, metaData = load_all_data()\n",
    "\n",
    "# Clean data\n",
    "train_labels, train_targets, test_labels = clean_data(train, test, features, float_numerical_features, int_numerical_features, cat_features, log_targets=False, log_area=True, fillNan=True)\n",
    "\n",
    "# Add result feature engineering\n",
    "train_labels, test_labels, added_features = feature_engineering(\n",
    "    train_labels, \n",
    "    test_labels,\n",
    "    add_base_features=True, \n",
    "    add_bool_features=True,\n",
    "    add_weak_features=True,\n",
    "    add_dist_to_metro=True,\n",
    "    add_close_to_uni=True,\n",
    "    add_dist_to_hospital=True,\n",
    "    add_floor_features=True,\n",
    "    add_street_info=True,\n",
    "    add_some_more_features=True,\n",
    "    add_district_information=True,\n",
    "    )\n",
    "\n",
    "# Normalize\n",
    "train_labels, test_labels = normalize(train_labels, test_labels, float_numerical_features, scaler=\"minMax\")\n",
    "\n",
    "# One-hot encoding\n",
    "train_labels_one_hot, test_labels_one_hot = one_hot_encoder(train_labels, test_labels, [\"condition\", \"district\", \"material\", \"parking\", \"heating\", \"seller\"], drop_old=True)\n",
    "\n",
    "# Drop unnecessary features\n",
    "droptable = ['street','address']\n",
    "train_labels.drop(droptable, inplace=True, axis=1)\n",
    "test_labels.drop(droptable, inplace=True, axis=1)\n",
    "train_labels_one_hot.drop(droptable, inplace=True, axis=1)\n",
    "test_labels_one_hot.drop(droptable, inplace=True, axis=1)\n",
    "\n",
    "# Categorical features to integers (necessary for CatBoost)\n",
    "cat_dict = {}\n",
    "for feature in cat_features:\n",
    "    cat_dict[feature] = 'int'\n",
    "train_labels = train_labels.astype(cat_dict)\n",
    "test_labels = test_labels.astype(cat_dict)\n",
    "\n",
    "print(\"You have now imported all data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497193f-9b8d-493c-96a7-c64195145090",
   "metadata": {},
   "source": [
    "### Preparing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419706b-f99e-4a9a-9424-e6b8a3dcbb58",
   "metadata": {},
   "source": [
    "#### Global Parameters and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88908ef6-60f3-47c9-9505-9ed0fe714d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "price_per_square_meter = train_targets/train['area_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602d58e-157a-4203-abfa-8dc905778ca0",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7534ce-acae-48e6-bfc5-1866ee6426b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\n",
    " 'num_iterations': 10000,\n",
    " 'n_estimators': 152*5,\n",
    " 'learning_rate': 0.05/5,\n",
    " 'num_leaves': 40,\n",
    " 'max_depth': 10,\n",
    " 'min_data_in_leaf': 20,\n",
    " 'bagging_fraction': 0.9,\n",
    " 'bagging_freq': 5,\n",
    " 'feature_fraction': 0.8,\n",
    "}\n",
    "lgb = lgbm.LGBMRegressor(\n",
    "    **params, \n",
    "    random_state= 1,\n",
    "    silent=True,\n",
    "    metric='regression',\n",
    "    num_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64de370-3cfe-4bb0-b716-941dc131c162",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7219bd59-6420-4b45-9a24-5dbf401791b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'learning_rate': 0.0772446776104594, \n",
    "    'l2_leaf_reg': 0.5709519938247928, \n",
    "    'colsample_bylevel': 0.09221911838854839, \n",
    "    'depth': 9, \n",
    "    'boosting_type': 'Plain', \n",
    "    'bootstrap_type': 'MVS', \n",
    "    'min_data_in_leaf': 17, \n",
    "    'one_hot_max_size': 11}\n",
    "catboost = CatBoostRegressor(\n",
    "    **best_params, \n",
    "    random_state=42,\n",
    "    loss_function='RMSE', \n",
    "    cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f40a84-6ff4-432c-805b-274f6fa5c75a",
   "metadata": {},
   "source": [
    "#### GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729603c7-cc63-4280-a702-9518636b2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_estimators = 300\n",
    "optimal_max_depth = 13\n",
    "optimal_min_samples_split = 1000\n",
    "optimal_min_samples_leaf = 40\n",
    "optimal_max_features = 40\n",
    "optimal_subsample = 0.95\n",
    "original_learning_rate = 0.1\n",
    "\n",
    "gradientboost = GradientBoostingRegressor(\n",
    "            n_estimators = optimal_n_estimators*10,\n",
    "            max_depth = optimal_max_depth,\n",
    "            min_samples_split = optimal_min_samples_split,\n",
    "            min_samples_leaf = optimal_min_samples_leaf,\n",
    "            max_features = optimal_max_features,\n",
    "            subsample = optimal_subsample,\n",
    "            learning_rate = original_learning_rate / 10,\n",
    "            loss = 'squared_error',\n",
    "            criterion = 'squared_error',\n",
    "            verbose = 0,\n",
    "            warm_start = False,\n",
    "            random_state = 42,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f847254-8591-4a1d-a7be-a12422cc16e8",
   "metadata": {},
   "source": [
    "### Training the Models\n",
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b6c3f80-6acf-4f62-8a72-03d9f1e3e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.fit(train_labels_one_hot.drop(['area_total','building_id'],axis=1),np.log(price_per_square_meter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414270a7-35b4-4082-ad04-964787412abd",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b52e89-1783-4629-bd99-c19acb20f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost.fit(train_labels.drop(['area_total','building_id'],axis=1),np.log(price_per_square_meter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443a9e1-b5e0-4de0-86c3-a841812a4ea6",
   "metadata": {},
   "source": [
    "#### GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "626baeab-9e43-4cf6-b3d5-15091efad785",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientboost.fit(train_labels_one_hot.drop(['area_total','building_id'],axis=1),np.log(price_per_square_meter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771c372-7dc8-47ed-8416-7b6696091921",
   "metadata": {},
   "source": [
    "### Predicting the Test Data\n",
    "#### LightGBM: public score `0.15653`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f93f2a02-c32f-4ebe-a197-37873064e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_store(lgb, test_labels_one_hot.drop(['area_total','building_id'],axis=1), test, path=\"LGBM_2_copy_2.csv\", exponential=True, price_per_sq = True, total_area_df = test['area_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9199db-6ddc-46b5-844c-255e4d0bd32e",
   "metadata": {},
   "source": [
    "Note that this recomputes the second submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556cbd4-69a3-46e6-ac33-cf31ae37c127",
   "metadata": {},
   "source": [
    "#### CatBoost: public score `0.16221`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9865aae8-3950-4dee-b1ec-880f0d86e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_store(catboost, test_labels.drop(['area_total','building_id'],axis=1), test, path=\"CatBoost_copy2.csv\", exponential=True, price_per_sq = True, total_area_df = test['area_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508216f-dcec-478b-b042-6a6c03c607da",
   "metadata": {},
   "source": [
    "#### GradientBoost: public score `0.16443`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63b61297-1ab4-4a93-85c7-587752697fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_store(gradientboost, test_labels_one_hot.drop(['area_total','building_id'],axis=1), test, path=\"GradientBoost_copy_3.csv\", exponential=True, price_per_sq = True, total_area_df = test['area_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8d2bf-383a-48e7-911f-5b3c0f607870",
   "metadata": {},
   "source": [
    "### Predicting the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baf0df70-1ab1-4824-b569-654caf691785",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def csv_bagging(kaggle_scores, csv_paths, submission_path):\n",
    "    # Making the acc dataframe\n",
    "    d = {}\n",
    "    for i, score in enumerate(kaggle_scores):\n",
    "        d[i] = score\n",
    "    acc = pd.DataFrame(\n",
    "    d,\n",
    "    index=[0]\n",
    "    )\n",
    "    acc = acc.T\n",
    "    acc.columns = ['RMSLE']\n",
    "    acc\n",
    "\n",
    "    # Read dataframes, sort and store\n",
    "    pd_predictions = []\n",
    "    for path in csv_paths:\n",
    "        pd_predictions.append(\n",
    "            pd.read_csv(path).sort_values(by=\"id\")\n",
    "            )\n",
    "    # Cast to numpy\n",
    "    np_predictions = []\n",
    "    for pred in pd_predictions:\n",
    "        np_predictions.append(\n",
    "            pred[\"price_prediction\"].to_numpy().T\n",
    "        )\n",
    "\n",
    "    # Bagging\n",
    "    avg_prediction = np.average(\n",
    "        np_predictions,\n",
    "        weights = 1 / acc['RMSLE'] ** 4,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    result = avg_prediction\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = pd_predictions[0]['id']\n",
    "    submission['price_prediction'] = result\n",
    "    if len(submission['id']) != 9937:\n",
    "        raise Exception(\"Not enough rows submitted!\")\n",
    "    \n",
    "    submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91e12ef1-ddd9-4cae-977e-76d94bab628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = [0.15653, 0.16443, 0.16221]\n",
    "csv_paths = [\"LGBM_2_copy_2.csv\", \"GradientBoost_copy_2.csv\", \"CatBoost_copy2.csv\"]\n",
    "submission_path = \"Bagging_copy_2.csv\"\n",
    "\n",
    "csv_bagging(prediction_scores, csv_paths, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07210352-052f-43de-b207-e04725523da9",
   "metadata": {},
   "source": [
    "Note: in case one would want to reproduce our absolute best submission (using only previously stored csv files), this code could be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f62702-12a9-401a-b0d6-4f92505b512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = [0.15623, 0.16401, 0.16264]\n",
    "csv_paths = [\"LGBM_2_copy_2.csv\", \"GradientBoost_copy_2.csv\", \"CatBoost_copy2.csv\"]\n",
    "submission_path = \"Bagging_original.csv\"\n",
    "\n",
    "csv_bagging(prediction_scores, csv_paths, submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f595735-8044-4507-838e-078bbf046b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bf77f-f633-4005-9f0b-69db178cb8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
